{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentice Chef Case Study - Cross-Sell Success\n",
    "\n",
    "    Owner: Jason Lee\n",
    "    Date: March 14, 2020\n",
    "    Topic: Machine Learning\n",
    "    Cohort: Valencia\n",
    "\n",
    "## Company Information\n",
    " \n",
    "Apprentice Chef, Inc. is an innovative company with a unique spin on cooking at home. Developed for the busy professional that has little to no skills in the kitchen, they offer a wide selection of daily-prepared gourmet meals delivered directly to your door.\n",
    "\n",
    "Each meal set takes at most 30 minutes to finish cooking at home and also comes with Apprentice Chef's award- winning disposable cookware (i.e. pots, pans, baking trays, and utensils), allowing for fast and easy cleanup. Ordering meals is very easy given their user-friendly online platform and mobile app. \n",
    "\n",
    "## Objective\n",
    "In an effort to diversify their revenue stream, Apprentice Chef, Inc. has launched Halfway There, a cross-selling promotion where subscribers receive a half bottle of wine from a local California vineyard every Wednesday (halfway through the work week). The executives at Apprentice Chef also believe this endeavor will create a competitive advantage based on its unique product offering of hard to find local wines. \n",
    " \n",
    "Halfway There has been exclusively offered to all of the customers in the dataset you received, and the executives would like to promote this service to a wider audience. They have tasked you with analyzing their data, developing your top insights, and building a machine learning model to predict which customers will subscribe to this service. \n",
    "\n",
    "\n",
    "## Dataset Metadata\n",
    "Column\t|Description|\n",
    ":--|:--|\n",
    "REVENUE\t|Total revenue generated over the first year of a customer's journey|\n",
    "CROSS_SELL_SUCCESS\t|Success of promoting Halfway There (1 = SUCCESS, 0 = FAIL)|\n",
    "NAME|\tFull name of customer (collected upon initial registration)|\n",
    "EMAIL\t|Email of customer (collected upon initial registration)|\n",
    "FIRST_NAME\t|First name of customer (collected upon initial registration)|\n",
    "FAMILY_NAME\t|Last name of customer (collected upon initial registration)|\n",
    "TOTAL_MEALS_ORDERED|\tTotal count of meals ordered per customer account|\n",
    "UNIQUE_MEALS_PURCH|\tCount of unique meal sets ordered per customer account|\n",
    "CONTACTS_W_CUSTOMER_SERVICE\t|Count of times a customer made contact with customer service (phone, chatbot, or email)|\n",
    "PRODUCT_CATEGORIES_VIEWED\t|Total number of meal categories viewed (online and mobile platforms combined)|\n",
    "AVG_TIME_PER_SITE_VISIT\t|Average platform (web or mobile) visit time per customer account|\n",
    "MOBILE_NUMBER|\tCustomer registered with a mobile or landline number (1 = MOBILE, 0 = LANDLINE)|\n",
    "CANCELLATIONS_BEFORE_NOON\t|Number of meals canceled before 12 PM as per cancelation policy|\n",
    "CANCELLATIONS_AFTER_NOON|\tNumber of meals canceled after 3 PM as per cancelation policy|\n",
    "TASTES_AND_PREFERENCES\t|Customer specified their tastes and preferences in their profile|\n",
    "MOBILE_LOGINS|\tCount of logins on the mobile platform (app)|\n",
    "PC_LOGINS|\tCount of logins on the web platform (website)|\n",
    "WEEKLY_PLAN|\tCount of weeks a customer subscribed to the weekly plan|\n",
    "EARLY_DELIVERIES\t|Count of orders that we delivered BEFORE the alloted delivery time|\n",
    "LATE_DELIVERIES\t|Count of orders that we delivered AFTER the alloted delivery time|\n",
    "PACKAGE_LOCKER|\tCustomer's building has a package locker service or package room|\n",
    "REFRIGERATED_LOCKER|\tPackage room has a refrigerated locker|\n",
    "FOLLOWED_RECOMMENDATIONS_PCT\t|Percentage of time a customer followed meal recommendations generated displayed on the web or mobile platform|\n",
    "AVG_PREP_VID_TIME|\tAverage time in seconds a customer watched  instructional videos for meal preparation|\n",
    "LARGEST_ORDER_SIZE\t|Largest number of meals a customer has ordered in a single order|\n",
    "MASTER_CLASSES_ATTENDED|\tCount of times a customer attended master class (learning to cook)|\n",
    "MEDIAN_MEAL_RATING|\tMedian meal satisfaction rating by customer|\n",
    "AVG_CLICKS_PER_VISIT|\tAverage number of clicks per site visit|\n",
    "TOTAL_PHOTOS_VIEWED|\tCount of photos viewed on web and mobile platforms (measured in clicks)|\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "## Report Overview\n",
    "\n",
    "    1. Exploration\n",
    "        - Missing values\n",
    "        - Descriptive statistics\n",
    "        - Potential outliers\n",
    "        - Cross Sell Success\n",
    "            - Distribution\n",
    "            - Median Values\n",
    "            - Correlation\n",
    "    \n",
    "    2. Feature Engineering\n",
    "        - Additional Calculated Fields\n",
    "        - Log\n",
    "        - Outlier Detection\n",
    "        - Trend Analysis\n",
    "        - Potential Youth\n",
    "        - Family Name\n",
    "        - Number of Names\n",
    "        - Full Name\n",
    "        - Family Name\n",
    "        - Discount Plan\n",
    "        - Email Domains\n",
    "        - Median Rating\n",
    "\n",
    "    3. Dataset Preparation\n",
    "        - Dropping Categorical Values\n",
    "        - Updated Correlation\n",
    "        \n",
    "    4. Variable Selection\n",
    "        - Stats Model\n",
    "            - All Explanatory Variables\n",
    "            - Adjusted Model\n",
    "        - Pruned Tree\n",
    "        \n",
    "    5. Model Building\n",
    "    \n",
    "        - Logistic Regression\n",
    "        - KNN Non Standardized\n",
    "        - Classification Tree - Pruned\n",
    "        - Support Vector Machine (SVM)\n",
    "        - Bayes Model\n",
    "        - Random Forest\n",
    "        \n",
    "    6. Model Performance\n",
    "    \n",
    "    7. Project Overview\n",
    "        - Insights\n",
    "        - Recommendations\n",
    "        \n",
    "    8. Sources\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "\n",
    "# 1. Exploratory Analysis\n",
    "In this part of the analysis we will explore the types of data we have, look for any outliers or interesting information we could use further in the report and study if there are any relationships between the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import random            as rand                     # random number gen\n",
    "import pandas            as pd                       # data science essentials\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import seaborn           as sns   \n",
    "import numpy            as np # enhanced data viz\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "\n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "# soft code file name\n",
    "file = \"Apprentice_Chef_Dataset.xlsx\"\n",
    "\n",
    "#dataframe\n",
    "original_df = pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# original_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "There are a total 1946 variables in the dataset, so any variables with less than this will have missing values.\n",
    "\n",
    "Variable  | Total Variables |\n",
    "-|--|\n",
    "FAMILY_NAME | 1899 |\n",
    "\n",
    "Since FAMILY_NAME is the only one missing values (1899 out of 1949), we can create a missing value column but since there is only 47 values missing it won't make a significant impact. FAMILY_NAME could also be a redundant column since we have the full name. We will make a note of this and make sure to check if there is value from FAMILY_NAME when we do feature engineering further down in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "\n",
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>MOBILE_NUMBER</th>\n",
       "      <th>CANCELLATIONS_BEFORE_NOON</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>TASTES_AND_PREFERENCES</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>EARLY_DELIVERIES</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>PACKAGE_LOCKER</th>\n",
       "      <th>REFRIGERATED_LOCKER</th>\n",
       "      <th>FOLLOWED_RECOMMENDATIONS_PCT</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>MASTER_CLASSES_ATTENDED</th>\n",
       "      <th>MEDIAN_MEAL_RATING</th>\n",
       "      <th>AVG_CLICKS_PER_VISIT</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2107.29</td>\n",
       "      <td>0.68</td>\n",
       "      <td>74.63</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.98</td>\n",
       "      <td>5.38</td>\n",
       "      <td>99.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.71</td>\n",
       "      <td>5.52</td>\n",
       "      <td>1.48</td>\n",
       "      <td>11.33</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.11</td>\n",
       "      <td>35.41</td>\n",
       "      <td>150.56</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.79</td>\n",
       "      <td>13.51</td>\n",
       "      <td>106.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1138.29</td>\n",
       "      <td>0.47</td>\n",
       "      <td>55.31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.28</td>\n",
       "      <td>3.04</td>\n",
       "      <td>62.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>13.57</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>26.58</td>\n",
       "      <td>49.45</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.33</td>\n",
       "      <td>181.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>131.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1350.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>114.40</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1740.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>94.16</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>145.60</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2670.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>117.29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>173.78</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>174.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8793.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1645.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>564.20</td>\n",
       "      <td>11.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1600.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       REVENUE  CROSS_SELL_SUCCESS  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  MOBILE_NUMBER  CANCELLATIONS_BEFORE_NOON  CANCELLATIONS_AFTER_NOON  TASTES_AND_PREFERENCES  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  EARLY_DELIVERIES  LATE_DELIVERIES  PACKAGE_LOCKER  REFRIGERATED_LOCKER  FOLLOWED_RECOMMENDATIONS_PCT  AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  MASTER_CLASSES_ATTENDED  MEDIAN_MEAL_RATING  AVG_CLICKS_PER_VISIT  TOTAL_PHOTOS_VIEWED\n",
       "count  1946.00             1946.00              1946.00              1946.0                      1946.00                    1946.00                  1946.00        1946.00                    1946.00                   1946.00                 1946.00    1946.00        1946.00      1946.00           1946.00          1946.00         1946.00              1946.00                       1946.00            1946.00             1946.00                  1946.00             1946.00               1946.00              1946.00\n",
       "mean   2107.29                0.68                74.63                 4.9                         6.98                       5.38                    99.60           0.88                       1.40                      0.17                    0.71       5.52           1.48        11.33              1.49             2.97            0.36                 0.11                         35.41             150.56                4.44                     0.60                2.79                 13.51               106.43\n",
       "std    1138.29                0.47                55.31                 2.5                         2.28                       3.04                    62.34           0.33                       1.55                      0.43                    0.45       0.58           0.53        13.57              2.32             2.74            0.48                 0.32                         26.58              49.45                1.55                     0.64                0.76                  2.33               181.01\n",
       "min     131.00                0.00                11.00                 1.0                         1.00                       1.00                    10.33           0.00                       0.00                      0.00                    0.00       4.00           0.00         0.00              0.00             0.00            0.00                 0.00                          0.00              33.40                0.00                     0.00                1.00                  5.00                 0.00\n",
       "25%    1350.00                0.00                39.00                 3.0                         5.00                       3.00                    72.00           1.00                       0.00                      0.00                    0.00       5.00           1.00         1.00              0.00             1.00            0.00                 0.00                         10.00             114.40                3.00                     0.00                2.00                 12.00                 0.00\n",
       "50%    1740.00                1.00                60.00                 5.0                         7.00                       5.00                    94.16           1.00                       1.00                      0.00                    1.00       6.00           1.00         7.00              0.00             2.00            0.00                 0.00                         30.00             145.60                4.00                     1.00                3.00                 13.00                 0.00\n",
       "75%    2670.00                1.00                95.00                 7.0                         8.00                       8.00                   117.29           1.00                       2.00                      0.00                    1.00       6.00           2.00        13.00              3.00             4.00            1.00                 0.00                         60.00             173.78                5.00                     1.00                3.00                 15.00               174.00\n",
       "max    8793.75                1.00               493.00                19.0                        18.00                      10.00                  1645.60           1.00                      13.00                      3.00                    1.00       7.00           3.00        52.00              9.00            19.00            1.00                 1.00                         90.00             564.20               11.00                     3.00                5.00                 19.00              1600.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics\n",
    "original_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "  - Possible binary options - CROSS_SELL_SUCCESS, MOBILE_NUMBER, TASTES_AND_PREFERENCES, PACKAGE_LOCKER, REFRIGERATED_LOCKER\n",
    "  - max REVENUE is significantly higher that mean and median values - 8793.75\n",
    "  - TOTAL_MEALS_ORDERED - almost 500 orders for a single customer - 493\n",
    "  - AVG_TIME_PER_SITE_VISIT, one customer visits the site 1645.60\n",
    "  - WEEKLY_PLAN, a customer orders 52 meals a week - interesting, normally 3 meals a day on average = 28 meals a week.\n",
    "  - AVG_PREP_VID_TIME, max time a customer watches videos is 564.20 - abnormal compared to others.\n",
    "  - TOTAL_PHOTOS_VIEWED, very active customer viewing photos - 1600\n",
    "\n",
    "Looking at all these numbers is clear that there is a lot of suspicious MAX values for a variety of columns, which will be worth checking if they belong to the same individual or multiple.\n",
    "\n",
    "### Checking Outliers\n",
    "To get a better understanding on the users, below explores if there are any repetitive users or patterns that are standing out in more than one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# outliers\n",
    "# # check max revenue user\n",
    "# print(original_df[original_df[\"REVENUE\"]==8793.75]['NAME'])\n",
    "\n",
    "# # check max orders user\n",
    "# print(original_df[original_df[\"TOTAL_MEALS_ORDERED\"]==493]['NAME'])\n",
    "\n",
    "# # check max orders user\n",
    "# print(original_df[original_df[\"TOTAL_MEALS_ORDERED\"]==493]['NAME'])\n",
    "\n",
    "# # check max weekly plan user\n",
    "# print(original_df[original_df[\"WEEKLY_PLAN\"]==52]['NAME'])\n",
    "\n",
    "# # check max vide watching time user\n",
    "# print(original_df[original_df[\"AVG_PREP_VID_TIME\"]==564.20]['NAME'])\n",
    "\n",
    "# # check max photo viewed time user\n",
    "# print(original_df[original_df[\"TOTAL_PHOTOS_VIEWED\"]==1600]['NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the analysis above are:\n",
    "\n",
    "    - Leyla Hightower is the user with the highest revenue at $8793\n",
    "    - Steffon Baratheon is the user with the max orders at 493\n",
    "    - Preston Greenfield has spent the most time visit Apprentice Chef with 1645.60 mins\n",
    "    - There are 17 users that all have the maximum weekly plan of 52 meals\n",
    "    - Turnip has spend the most time watching video with 564.20 mins\n",
    "    - Maron Greyjoy has viewed the most photos by a significant amount with 1600 photos viewed.\n",
    "\n",
    "After looking into each abnormal pattern in potential outliers, none of our customers appeared more than once. This tells us there is no overlap in having a single user skewing all the results.\n",
    "\n",
    "\n",
    "Breaking down the dataset, we can see that several customers have high numbers for values that you would assume would positively affect revenue, but it did not. For example, despite the customer with the most orders (493 orders), 52 other customers had more revenue. With this information, it would signal substantial discounts or inconsistent pricing. Similarly, we see the customer with the highest amount of time spent on the website with 2685 minutes, had less revenue than 480 customers (almost 25% of the customer base).\n",
    "\n",
    "\n",
    "Let's explore potential outliers further by graphing the variables and spot any meaning full patterns.\n",
    "\n",
    "## Exploring Cross Sell Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot Pie Chart for transporation\n",
    "plt.figure(figsize=(5,5))\n",
    "labels = (\"Success 68%\" , \"Failure 32%\",)\n",
    "explode = (0.1,0)\n",
    "original_df['CROSS_SELL_SUCCESS'].value_counts().plot.pie(fontsize = 16, explode = explode, counterclock=False, labeldistance=1.1,radius = 1,labels = labels)\n",
    "plt.title(\"Cross Sell Success\", fontsize = 20)\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Customer information are the only categorical values - NAME, EMAIL, FIRST_NAME, & FAMILY_NAME\n",
    " - Only missing values are related to FAMILY_NAME (47 missing total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Values Comparison\n",
    "Below we can compare the median values for selected variables, based on whether there was a successful cross sell promotion for the wine campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# original_df[original_df['CROSS_SELL_SUCCESS'] ==1].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# original_df[original_df['CROSS_SELL_SUCCESS'] ==0].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable | Cross Sell Success | Cross Sell Failure |\n",
    "-------------| ----------| ---------------|\n",
    "REVENUE                        | 1750.00 | 1725.00\n",
    "TOTAL_MEALS_ORDERED            |  61.00  | 57.00\n",
    "CONTACTS_W_CUSTOMER_SERVICE    |    7.00  | 7.00\n",
    "AVG_TIME_PER_SITE_VISIT        |  94.5  | 93.3\n",
    "AVG_PREP_VID_TIME              |  147.4  | 143.00\t\n",
    "LARGEST_ORDER_SIZE             |    4.00  | 4.00\t\n",
    "MEDIAN_MEAL_RATING             |    3.00  | 3.00\n",
    "AVG_CLICKS_PER_VISIT           |    13.0      | 14.0\n",
    "TOTAL_PHOTOS_VIEWED            |  300.00  | 0.00\n",
    "\n",
    "- We can see that comparing success and failure for cross sell has slight difference in selected median values.\n",
    "- Exploring the overview of the dataset we can start thinking of ways to manufacture features that can improve the relationship and intuition of the Cross Sell Success.\n",
    "\n",
    "### Correlation\n",
    "We can explore the correlation results between the variables and our response, CROSS SELL SUCCESS to determine if there is any strong insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# correaltion\n",
    "# corr_original = original_df.corr().round(2)\n",
    "\n",
    "# corr_original['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Top Correlation Results of Cross Sell Success</center>\n",
    "\n",
    "Variable | Correlation\n",
    "----|---|\n",
    "FOLLOWED_RECOMMENDATIONS_PCT    |   0.46|\n",
    "CANCELLATIONS_BEFORE_NOON        |  0.16|\n",
    "MOBILE_NUMBER                     | 0.10|\n",
    "TASTES_AND_PREFERENCES            | 0.08|\n",
    "REFRIGERATED_LOCKER              | 0.07 |\n",
    "\n",
    "Followed recommendation has a decent size correlation at 0.45. The represents the percentage of time a customer followed meal recommendations generated displayed on the web or mobile platform. It could mean that customers who are being recommend meals are more open to clicking on promotions, which would make sense as to why they are open to the wine cross sell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "Our goal is to develop the best model to help predict the amount of revenue a customer will bring into the company in their first year. In order to accomplish this we will need to engineer additional features to help enhance the model's performance. We are given a set amount of data inputs but in order to have a more robust model, we need to develop intuition and strategies for new features.\n",
    "\n",
    "\n",
    "<i> The features in your data will directly influence the predictive models you use and the results you can achieve. </i> (Source: [Brownlee (2019) Machine Learning Mastery](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/))\n",
    "\n",
    "\n",
    "Before developing new features let's breakdown the types of variables we have.\n",
    "\n",
    "\n",
    "BINARY:\n",
    "\n",
    "    - CROSS_SELL_SUCCESS\t\n",
    "    - TASTES_AND_PREFERENCES\n",
    "    - PACKAGE_LOCKER\n",
    "    - REFRIGERATED_LOCKER\n",
    "\n",
    "CATEGORICAL:\n",
    "\n",
    "    - MOBILE_NUMBER\n",
    "    \n",
    "CONTINUOUS OR INTERVAL:\n",
    "\n",
    "    - REVENUE\n",
    "    - AVG_TIME_PER_SITE_VISIT\n",
    "    - AVG_PREP_VID_TIME\t\n",
    "\n",
    "COUNT:\n",
    "\n",
    "    - TOTAL_MEALS_ORDERED\n",
    "    - UNIQUE_MEALS_PURCH\n",
    "    - CONTACTS_W_CUSTOMER_SERVICE\n",
    "    - PRODUCT_CATEGORIES_VIEWED\n",
    "    - CANCELLATIONS_BEFORE_NOON\n",
    "    - CANCELLATIONS_AFTER_NOON\n",
    "    - PC_LOGINS\t\n",
    "    - MOBILE_LOGINS\n",
    "    - WEEKLY_PLAN\t\n",
    "    - EARLY_DELIVERIES\t\n",
    "    - LATE_DELIVERIES\n",
    "    - FOLLOWED_RECOMMENDATIONS_PCT\n",
    "    - LARGEST_ORDER_SIZE\n",
    "    - MASTER_CLASSES_ATTENDED         \n",
    "    - MEDIAN_MEAL_RATING\n",
    "    - AVG_CLICKS_PER_VISIT\n",
    "    - TOTAL_PHOTOS_VIEWED\n",
    "    \n",
    "DISCRETE:\n",
    "\n",
    "    - NAME                           \n",
    "    - EMAIL                           \n",
    "    - FIRST_NAME                      \n",
    "    - FAMILY_NAME\n",
    "\n",
    "---------------------------\n",
    "\n",
    "## Feature - Additional Calculated Fields\n",
    "Using Revenue to determine the per unit impact for clicks and total meals can provide more insight into granular impact. Similarly we can take the total cancellations as a single variable by combining cancellations before and after noon. Using this will provide more information on the effects of variables we currently have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated fields\n",
    "original_df['REVENUE_PER_CLICK'] = original_df['REVENUE'] /original_df['AVG_CLICKS_PER_VISIT']\n",
    "original_df['REVENUE_PER_ORDER'] = original_df['REVENUE'] /original_df['TOTAL_MEALS_ORDERED']\n",
    "original_df['TOTAL_CANCELLATIONS'] = original_df['CANCELLATIONS_BEFORE_NOON']  + original_df['CANCELLATIONS_AFTER_NOON']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------\n",
    "\n",
    "## Feature - Log\n",
    "In this step we will obtain the logs of selected variables. Since most of the variables are all set on different scales and units, using log reduces the possible range and prevents skewness. We get the logs to ensure we have consistent range of measurements and it will add more stability to our model. The benefits of using log include the following:\n",
    "\n",
    "    - Manages skewed data\n",
    "    - Distribution resembles Gaussian\n",
    "    - Decreases effects from outliers\n",
    "    - More robust model\n",
    "    \n",
    "Source: [Enberolu (2019) - Towards Data Science](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to get the log of select columns\n",
    "log_list = ['TOTAL_MEALS_ORDERED','UNIQUE_MEALS_PURCH','CONTACTS_W_CUSTOMER_SERVICE',\n",
    "            'PRODUCT_CATEGORIES_VIEWED','AVG_TIME_PER_SITE_VISIT','PC_LOGINS',\n",
    "            'AVG_PREP_VID_TIME','MEDIAN_MEAL_RATING','AVG_CLICKS_PER_VISIT']\n",
    "\n",
    "for column in log_list:\n",
    "    original_df['log_'+ column] = original_df[column].transform(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "## Feature - Outlier Detection (Thresholds)\n",
    "As we mentioned earlier there were several customers that flagged some potential outliers in the exploration page. To address this, we can use normal distribution graphs to identify, and use the insights to set threshold boundaries for creating outlier specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # histograms\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "# plt.subplot(3, 3, 1)\n",
    "# sns.distplot(original_df['TOTAL_MEALS_ORDERED'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'orange')\n",
    "# plt.xlabel('Total Meals')\n",
    "\n",
    "# ########################\n",
    "# plt.subplot(3, 3, 2)\n",
    "# sns.distplot(original_df['UNIQUE_MEALS_PURCH'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'r')\n",
    "# plt.xlabel('Unique Meals Purchased')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 3)\n",
    "# sns.distplot(original_df['CONTACTS_W_CUSTOMER_SERVICE'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'g')\n",
    "# plt.xlabel('Customer Service Contact')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 4)\n",
    "# sns.distplot(original_df['PRODUCT_CATEGORIES_VIEWED'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'y')\n",
    "# plt.xlabel('Product Categories')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 5)\n",
    "# sns.distplot(original_df['AVG_TIME_PER_SITE_VISIT'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'orange')\n",
    "# plt.xlabel('Avg Time Visit')\n",
    "\n",
    "# ##########################\n",
    "\n",
    "# plt.subplot(3, 3, 6)\n",
    "# sns.distplot(original_df['CANCELLATIONS_BEFORE_NOON'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'y')\n",
    "# plt.xlabel('Before 12pm Cancel')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# ########################\n",
    "# ########################\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "\n",
    "# plt.subplot(3, 3, 1)\n",
    "# sns.distplot(original_df['CANCELLATIONS_AFTER_NOON'],\n",
    "#              bins  = 'fd',\n",
    "#              kde   = False,\n",
    "#              rug   = True,\n",
    "#              color = 'orange')\n",
    "# plt.xlabel('After 12pm Cancel')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 2)\n",
    "# sns.distplot(original_df['PC_LOGINS'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'g')\n",
    "# plt.xlabel('PC Logins')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 3)\n",
    "# sns.distplot(original_df['WEEKLY_PLAN'],\n",
    "#              bins = 10,\n",
    "#              color = 'orange')\n",
    "# plt.xlabel('Weekly Plan')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 4)\n",
    "# sns.distplot(original_df['EARLY_DELIVERIES'],\n",
    "#              bins = 'fd',\n",
    "#              rug  = True,\n",
    "#              color = 'r')\n",
    "# plt.xlabel('Early Delivery')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 5)\n",
    "# sns.distplot(original_df['LATE_DELIVERIES'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'g')\n",
    "# plt.xlabel('Late Delivery')\n",
    "\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 6)\n",
    "# sns.distplot(original_df['FOLLOWED_RECOMMENDATIONS_PCT'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'g')\n",
    "# plt.xlabel('Followed Recommendations')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# ########################\n",
    "# ########################\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "\n",
    "# plt.subplot(3, 3, 1)\n",
    "# sns.distplot(original_df['AVG_PREP_VID_TIME'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'y')\n",
    "# plt.xlabel('Avg Prep Video Time')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 2)\n",
    "# sns.distplot(original_df['LARGEST_ORDER_SIZE'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'orange')\n",
    "# plt.xlabel('Largest Order Size')\n",
    "\n",
    "# ########################\n",
    "# plt.subplot(3, 3, 3)\n",
    "# sns.distplot(original_df['MASTER_CLASSES_ATTENDED'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'r')\n",
    "# plt.xlabel('Master Classes Attened')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 4)\n",
    "# sns.distplot(original_df['MEDIAN_MEAL_RATING'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'g')\n",
    "# plt.xlabel('Median Meal Rating')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 5)\n",
    "# sns.distplot(original_df['AVG_CLICKS_PER_VISIT'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'y')\n",
    "# plt.xlabel('Avg Clicks Per Visit')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 6)\n",
    "# sns.distplot(original_df['TOTAL_PHOTOS_VIEWED'],\n",
    "#              bins  = 'fd',\n",
    "#              color = 'orange')\n",
    "# plt.xlabel('Total Photos Video')\n",
    "# plt.tight_layout()\n",
    "# ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setting thresholds for features\n",
    "TOTAL_MEALS_ORDERED_hi           = 150         # we can see a normal distribution if we cut of at this point\n",
    "UNIQUE_MEALS_PURCH_hi            = 8           # we can see a normal distribution if we cut of at this point\n",
    "CONTACTS_W_CUSTOMER_SERVICE_hi   = 9           # we can see a normal distribution if we cut of at this point\n",
    "PRODUCT_CATEGORIES_VIEWED_lo     = 2           # setting this as MIN will exclude outliers\n",
    "PRODUCT_CATEGORIES_VIEWED_hi     = 8           # setting this as MAX will exclude outliers\n",
    "AVG_TIME_PER_SITE_VISIT_hi       = 175         # we can see a normal distribution if we cut of at this point\n",
    "CANCELLATIONS_BEFORE_NOON_hi     = 3           # setting this as MIN will exclude outliers\n",
    "CANCELLATIONS_AFTER_NOON_hi      = 0           # setting this as MIN will exclude outliers\n",
    "PC_LOGINS_lo                     = 5           # setting this as MIN will exclude outliers\n",
    "PC_LOGINS_hi                     = 6           # setting this as MAX will exclude outliers\n",
    "WEEKLY_PLAN_hi                   = 18          # we can see a normal distribution if we cut of at this point\n",
    "EARLY_DELIVERIES_hi              = 2           # we can see a normal distribution if we cut of at this point\n",
    "LATE_DELIVERIES_hi               = 7           # we can see a normal distribution if we cut of at this point\n",
    "AVG_PREP_VID_TIME_hi             = 250         # we can see a normal distribution if we cut of at this point\n",
    "LARGEST_ORDER_SIZE_lo            = 3           # setting this as MIN will exclude outliers\n",
    "LARGEST_ORDER_SIZE_hi            = 6           # setting this as MAX will exclude outliers  \n",
    "MASTER_CLASSES_ATTENDED_hi       = 1           # we can see a normal distribution if we cut of at this point\n",
    "MEDIAN_MEAL_RATING_hi            = 3           # we can see a normal distribution if we cut of at this point\n",
    "AVG_CLICKS_PER_VISIT_lo          = 9           # setting this as MIN will exclude outliers\n",
    "AVG_CLICKS_PER_VISIT_hi          = 17.5        # setting this as MAX will exclude outliers \n",
    "TOTAL_PHOTOS_VIEWED_hi           = 50          # we can see a normal distribution if we cut of at this point\n",
    "\n",
    "# Feature Engineering (outlier thresholds)\n",
    "###########################################\n",
    "\n",
    "\n",
    "# total meals ordered\n",
    "original_df['out_TOTAL_MEALS_ORDERED'] = 0\n",
    "total_hi = original_df.loc[0:,'out_TOTAL_MEALS_ORDERED'][original_df['TOTAL_MEALS_ORDERED'] > TOTAL_MEALS_ORDERED_hi]\n",
    "\n",
    "original_df['out_TOTAL_MEALS_ORDERED'].replace(to_replace = total_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# unique meals purchase\n",
    "original_df['out_UNIQUE_MEALS_PURCH'] = 0\n",
    "unique_hi = original_df.loc[0:,\"out_UNIQUE_MEALS_PURCH\"][original_df['UNIQUE_MEALS_PURCH'] > UNIQUE_MEALS_PURCH_hi]\n",
    "\n",
    "original_df['out_UNIQUE_MEALS_PURCH'].replace(to_replace = unique_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# customer service contacts\n",
    "original_df['out_CONTACTS_W_CUSTOMER_SERVICE'] = 0\n",
    "customer_service_hi = original_df.loc[0:,\"out_CONTACTS_W_CUSTOMER_SERVICE\"][original_df['CONTACTS_W_CUSTOMER_SERVICE'] > CONTACTS_W_CUSTOMER_SERVICE_hi]\n",
    "\n",
    "original_df['out_CONTACTS_W_CUSTOMER_SERVICE'].replace(to_replace = customer_service_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# product categories viewed\n",
    "original_df['out_PRODUCT_CATEGORIES_VIEWED'] = 0\n",
    "product_hi = original_df.loc[0:,'out_PRODUCT_CATEGORIES_VIEWED'][original_df['PRODUCT_CATEGORIES_VIEWED'] > PRODUCT_CATEGORIES_VIEWED_hi]\n",
    "product_lo = original_df.loc[0:,'out_PRODUCT_CATEGORIES_VIEWED'][original_df['PRODUCT_CATEGORIES_VIEWED'] < PRODUCT_CATEGORIES_VIEWED_lo]\n",
    "\n",
    "original_df['out_PRODUCT_CATEGORIES_VIEWED'].replace(to_replace = product_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "original_df['out_PRODUCT_CATEGORIES_VIEWED'].replace(to_replace = product_lo,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# average visit time\n",
    "original_df['out_AVG_TIME_PER_SITE_VISIT'] = 0\n",
    "avg_time_hi = original_df.loc[0:,\"out_AVG_TIME_PER_SITE_VISIT\"][original_df['AVG_TIME_PER_SITE_VISIT'] > AVG_TIME_PER_SITE_VISIT_hi]\n",
    "\n",
    "original_df['out_AVG_TIME_PER_SITE_VISIT'].replace(to_replace = avg_time_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# before noon cancels\n",
    "original_df['out_CANCELLATIONS_BEFORE_NOON'] = 0\n",
    "before_noon_hi = original_df.loc[0:,\"out_CANCELLATIONS_BEFORE_NOON\"][original_df['CANCELLATIONS_BEFORE_NOON'] > CANCELLATIONS_BEFORE_NOON_hi]\n",
    "\n",
    "original_df['out_CANCELLATIONS_BEFORE_NOON'].replace(to_replace = before_noon_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# after noon cancels\n",
    "original_df['out_CANCELLATIONS_AFTER_NOON'] = 0\n",
    "after_noon_hi = original_df.loc[0:,\"out_CANCELLATIONS_AFTER_NOON\"][original_df['CANCELLATIONS_AFTER_NOON'] > CANCELLATIONS_AFTER_NOON_hi]\n",
    "\n",
    "original_df['out_CANCELLATIONS_AFTER_NOON'].replace(to_replace = after_noon_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# product categories viewed\n",
    "original_df['out_PC_LOGINS'] = 0\n",
    "pc_logins_hi = original_df.loc[0:,'out_PC_LOGINS'][original_df['PC_LOGINS'] > PC_LOGINS_hi]\n",
    "pc_logins_lo = original_df.loc[0:,'out_PC_LOGINS'][original_df['PC_LOGINS'] < PC_LOGINS_lo]\n",
    "\n",
    "original_df['out_PC_LOGINS'].replace(to_replace = pc_logins_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "original_df['out_PC_LOGINS'].replace(to_replace = pc_logins_lo,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# weekly plan\n",
    "original_df['out_WEEKLY_PLAN'] = 0\n",
    "weekly_hi = original_df.loc[0:,\"out_WEEKLY_PLAN\"][original_df['WEEKLY_PLAN'] > WEEKLY_PLAN_hi]\n",
    "\n",
    "original_df['out_WEEKLY_PLAN'].replace(to_replace = weekly_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# early delivery\n",
    "original_df['out_EARLY_DELIVERIES'] = 0\n",
    "early_hi = original_df.loc[0:,\"out_EARLY_DELIVERIES\"][original_df['EARLY_DELIVERIES'] > EARLY_DELIVERIES_hi]\n",
    "\n",
    "original_df['out_EARLY_DELIVERIES'].replace(to_replace = early_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# late delivery\n",
    "original_df['out_LATE_DELIVERIES'] = 0\n",
    "late_hi = original_df.loc[0:,\"out_LATE_DELIVERIES\"][original_df['LATE_DELIVERIES'] > LATE_DELIVERIES_hi]\n",
    "\n",
    "original_df['out_LATE_DELIVERIES'].replace(to_replace = late_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# avergae video time\n",
    "original_df['out_AVG_PREP_VID_TIME'] = 0\n",
    "avg_video_hi = original_df.loc[0:,\"out_AVG_PREP_VID_TIME\"][original_df['AVG_PREP_VID_TIME'] > AVG_PREP_VID_TIME_hi]\n",
    "\n",
    "original_df['out_AVG_PREP_VID_TIME'].replace(to_replace = avg_video_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# largest order size\n",
    "original_df['out_LARGEST_ORDER_SIZE'] = 0\n",
    "order_hi = original_df.loc[0:,'out_LARGEST_ORDER_SIZE'][original_df['LARGEST_ORDER_SIZE'] > LARGEST_ORDER_SIZE_hi]\n",
    "order_lo = original_df.loc[0:,'out_LARGEST_ORDER_SIZE'][original_df['LARGEST_ORDER_SIZE'] < LARGEST_ORDER_SIZE_lo]\n",
    "\n",
    "original_df['out_LARGEST_ORDER_SIZE'].replace(to_replace = order_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "original_df['out_LARGEST_ORDER_SIZE'].replace(to_replace = order_lo,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "\n",
    "# master classess attended\n",
    "original_df['out_MASTER_CLASSES_ATTENDED'] = 0\n",
    "master_hi = original_df.loc[0:,\"out_MASTER_CLASSES_ATTENDED\"][original_df['MASTER_CLASSES_ATTENDED'] > MASTER_CLASSES_ATTENDED_hi]\n",
    "\n",
    "original_df['out_MASTER_CLASSES_ATTENDED'].replace(to_replace = master_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# meal rating\n",
    "original_df['out_MEDIAN_MEAL_RATING'] = 0\n",
    "rating_hi = original_df.loc[0:,\"out_MEDIAN_MEAL_RATING\"][original_df['MEDIAN_MEAL_RATING'] > MEDIAN_MEAL_RATING_hi]\n",
    "\n",
    "original_df['out_MEDIAN_MEAL_RATING'].replace(to_replace = rating_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "\n",
    "# average clicks per visit\n",
    "original_df['out_AVG_CLICKS_PER_VISIT'] = 0\n",
    "clicks_hi = original_df.loc[0:,'out_AVG_CLICKS_PER_VISIT'][original_df['AVG_CLICKS_PER_VISIT'] > AVG_CLICKS_PER_VISIT_hi]\n",
    "clicks_lo = original_df.loc[0:,'out_AVG_CLICKS_PER_VISIT'][original_df['AVG_CLICKS_PER_VISIT'] < AVG_CLICKS_PER_VISIT_lo]\n",
    "\n",
    "original_df['out_AVG_CLICKS_PER_VISIT'].replace(to_replace = clicks_hi,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "original_df['out_AVG_CLICKS_PER_VISIT'].replace(to_replace = clicks_lo,\n",
    "                                 value      = 1,\n",
    "                                 inplace    = True)\n",
    "\n",
    "# total photos viewed\n",
    "original_df['out_TOTAL_PHOTOS_VIEWED'] = 0\n",
    "photos_hi = original_df.loc[0:,\"out_TOTAL_PHOTOS_VIEWED\"][original_df['TOTAL_PHOTOS_VIEWED'] > TOTAL_PHOTOS_VIEWED_hi]\n",
    "\n",
    "original_df['out_TOTAL_PHOTOS_VIEWED'].replace(to_replace = photos_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at each of these graphs, we want to make threshold minimum and maximum limits for each variable. Below are the threshold limits and reasoning as to why they were set.\n",
    "\n",
    "Now that we have our thresholds, we can set the parameters for building each new feature. We will focus on creating new features by tagging a number 1, if a variable is greater or less than the thresholds we set earlier. The format for outlier columns will follow the same format as previous example - out_column_name\"\n",
    "\n",
    "Example:\n",
    "\n",
    "    out_TOTAL_MEALS_ORDERED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "\n",
    "## Feature - Trend Analysis (Thresholds)\n",
    "Using scatter plots will help provide insights on if there are any clusters forming, and if we can identify any patterns outside of outliers. This process will require setting thresholds based on when items seem to be scattering more, or clustering more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # scatter plots trend analyssi \n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "# plt.subplot(3, 3, 1)\n",
    "# sns.scatterplot(x = original_df['TOTAL_MEALS_ORDERED'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'g')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 2)\n",
    "# sns.scatterplot(x = original_df['UNIQUE_MEALS_PURCH'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'y')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 3)\n",
    "# sns.scatterplot(x = original_df['CONTACTS_W_CUSTOMER_SERVICE'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'orange')\n",
    "\n",
    "# ########################\n",
    "# plt.subplot(3, 3, 4)\n",
    "# sns.scatterplot(x = original_df['PRODUCT_CATEGORIES_VIEWED'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'r')\n",
    "\n",
    "# #######################\n",
    "# plt.subplot(3, 3, 5)\n",
    "# sns.scatterplot(x = original_df['AVG_TIME_PER_SITE_VISIT'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'orange')\n",
    "\n",
    "# ########################\n",
    "# plt.subplot(3, 3, 6)\n",
    "# sns.scatterplot(x = original_df['CANCELLATIONS_BEFORE_NOON'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'r')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# ########################\n",
    "# ########################\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "# plt.subplot(3, 3, 1)\n",
    "# sns.scatterplot(x = original_df['CANCELLATIONS_AFTER_NOON'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'g')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 2)\n",
    "# sns.scatterplot(x = original_df['PC_LOGINS'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'y')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 3)\n",
    "# sns.scatterplot(x = original_df['WEEKLY_PLAN'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'orange')\n",
    "\n",
    "# ########################\n",
    "# plt.subplot(3, 3, 4)\n",
    "# sns.scatterplot(x = original_df['EARLY_DELIVERIES'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'r')\n",
    "\n",
    "# #######################\n",
    "# plt.subplot(3, 3, 5)\n",
    "# sns.scatterplot(x = original_df['LATE_DELIVERIES'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'orange')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 6)\n",
    "# sns.scatterplot(x = original_df['FOLLOWED_RECOMMENDATIONS_PCT'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'g')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# ########################\n",
    "# ########################\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "# plt.subplot(3, 3, 1)\n",
    "# sns.scatterplot(x = original_df['AVG_PREP_VID_TIME'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'r')\n",
    "\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 2)\n",
    "# sns.scatterplot(x = original_df['LARGEST_ORDER_SIZE'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'g')\n",
    "\n",
    "# ########################\n",
    "\n",
    "# plt.subplot(3, 3, 3)\n",
    "# sns.scatterplot(x = original_df['MASTER_CLASSES_ATTENDED'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'y')\n",
    "\n",
    "# ########################\n",
    "# plt.subplot(3, 3, 4)\n",
    "# sns.scatterplot(x = original_df['MEDIAN_MEAL_RATING'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'orange')\n",
    "\n",
    "# #######################\n",
    "# plt.subplot(3, 3, 5)\n",
    "# sns.scatterplot(x = original_df['AVG_CLICKS_PER_VISIT'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'r')\n",
    "\n",
    "\n",
    "# ######################\n",
    "# plt.subplot(3, 3, 6)\n",
    "# sns.scatterplot(x = original_df['TOTAL_PHOTOS_VIEWED'],\n",
    "#                 y = original_df['REVENUE'],\n",
    "#                 color = 'orange')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setting thresholds for features\n",
    "TOTAL_MEALS_ORDERED_hi           = 200        # plots begin to scatter\n",
    "CONTACTS_W_CUSTOMER_SERVICE_hi   = 9          # plots begin to scatter\n",
    "AVG_TIME_PER_SITE_VISIT_hi       = 200        # plots begin to scatter  \n",
    "LATE_DELIVERIES_hi               = 9          # plots begin to scatter\n",
    "AVG_PREP_VID_TIME_hi             = 225        # plots begin to scatter\n",
    "\n",
    "MASTER_CLASSES_ATTENDED_at       = 1          # one inflated\n",
    "UNIQUE_MEALS_PURCH_at            = 0          # zero inflated  \n",
    "CANCELLATIONS_AFTER_NOON_at      = 0          # zero inflated          \n",
    "WEEKLY_PLAN_at                   = 0          # zero inflated  \n",
    "MEDIAN_MEAL_RATING_at            = 3          # three inflated      \n",
    "TOTAL_PHOTOS_VIEWED_at           = 0          # zero inflated       \n",
    "\n",
    "###########################################\n",
    "## Feature Engineering (trends thresholds)\n",
    "###########################################\n",
    "\n",
    "\n",
    "# total meals ordered\n",
    "original_df['tre_TOTAL_MEALS_ORDERED'] = 0\n",
    "total_hi = original_df.loc[0:,'tre_TOTAL_MEALS_ORDERED'][original_df['TOTAL_MEALS_ORDERED'] > TOTAL_MEALS_ORDERED_hi]\n",
    "\n",
    "original_df['tre_TOTAL_MEALS_ORDERED'].replace(to_replace = total_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# # customer service contacts\n",
    "# original_df['tre_CONTACTS_W_CUSTOMER_SERVICE'] = 0\n",
    "# customer_service_hi = original_df.loc[0:,\"tre_CONTACTS_W_CUSTOMER_SERVICE\"][original_df['CONTACTS_W_CUSTOMER_SERVICE'] > CONTACTS_W_CUSTOMER_SERVICE_hi]\n",
    "\n",
    "# original_df['tre_CONTACTS_W_CUSTOMER_SERVICE'].replace(to_replace = customer_service_hi,\n",
    "#                                     value = 1,\n",
    "#                                     inplace = True)\n",
    "\n",
    "# after noon cancels\n",
    "original_df['tre_CANCELLATIONS_AFTER_NOON'] = 0\n",
    "after_noon_hi = original_df.loc[0:,\"tre_CANCELLATIONS_AFTER_NOON\"][original_df['CANCELLATIONS_AFTER_NOON'] == CANCELLATIONS_AFTER_NOON_at]\n",
    "\n",
    "original_df['tre_CANCELLATIONS_AFTER_NOON'].replace(to_replace = after_noon_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# weekly plan\n",
    "original_df['tre_WEEKLY_PLAN'] = 0\n",
    "weekly_hi = original_df.loc[0:,\"tre_WEEKLY_PLAN\"][original_df['WEEKLY_PLAN'] == WEEKLY_PLAN_at]\n",
    "\n",
    "original_df['tre_WEEKLY_PLAN'].replace(to_replace = weekly_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# late delivery\n",
    "original_df['tre_LATE_DELIVERIES'] = 0\n",
    "late_hi = original_df.loc[0:,\"tre_LATE_DELIVERIES\"][original_df['LATE_DELIVERIES'] > LATE_DELIVERIES_hi]\n",
    "\n",
    "original_df['tre_LATE_DELIVERIES'].replace(to_replace = late_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# avergae video time\n",
    "original_df['tre_AVG_PREP_VID_TIME'] = 0\n",
    "avg_video_hi = original_df.loc[0:,\"tre_AVG_PREP_VID_TIME\"][original_df['AVG_PREP_VID_TIME'] > AVG_PREP_VID_TIME_hi]\n",
    "\n",
    "original_df['tre_AVG_PREP_VID_TIME'].replace(to_replace = avg_video_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# master classess attended\n",
    "original_df['tre_MASTER_CLASSES_ATTENDED'] = 0\n",
    "master_hi = original_df.loc[0:,\"tre_MASTER_CLASSES_ATTENDED\"][original_df['MASTER_CLASSES_ATTENDED'] == MASTER_CLASSES_ATTENDED_at]\n",
    "\n",
    "original_df['tre_MASTER_CLASSES_ATTENDED'].replace(to_replace = master_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# meal rating\n",
    "original_df['tre_MEDIAN_MEAL_RATING'] = 0\n",
    "rating_hi = original_df.loc[0:,\"tre_MEDIAN_MEAL_RATING\"][original_df['MEDIAN_MEAL_RATING'] == MEDIAN_MEAL_RATING_at]\n",
    "\n",
    "original_df['tre_MEDIAN_MEAL_RATING'].replace(to_replace = rating_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "# total photos viewed\n",
    "original_df['tre_TOTAL_PHOTOS_VIEWED'] = 0\n",
    "photos_hi = original_df.loc[0:,\"tre_TOTAL_PHOTOS_VIEWED\"][original_df['TOTAL_PHOTOS_VIEWED'] > TOTAL_PHOTOS_VIEWED_at]\n",
    "\n",
    "original_df['tre_TOTAL_PHOTOS_VIEWED'].replace(to_replace = photos_hi,\n",
    "                                    value = 1,\n",
    "                                    inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to set these threshold to make sure we are accounting for any patterns in our customer's behavior. Making sure we cut any abnormal changes before they can skew our model performance. Following similar steps from previous examples, we will no build new features based on the threshold above - tagging either a 0 or 1 depending if the threshold boundary is met.\n",
    "\n",
    "The new threshold columns will be tagged with the format \"tre_column_name\"\n",
    "\n",
    "Example:\n",
    "\n",
    "    tre_LATE_DELIVERIES\n",
    "    \n",
    "Let's check the new columns are inputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check columns\n",
    "# original_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "## Feature - Potential Youth\n",
    "Looking in the the dataset we can inspect the NAME column and find a some users have the word \"son\" or \"daughter\" in their name. This could potential tell us if they are young, and gives us an opportunity to classify them as a potential youth category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# potential youth\n",
    "original_df['potential_youth'] = 0\n",
    "\n",
    "#looping to find youth\n",
    "for index, val in original_df.iterrows():\n",
    "    if 'son' in original_df.loc[ index , 'NAME']:\n",
    "        original_df.loc[index, 'potential_youth'] = 1\n",
    "    elif 'daughter' in original_df.loc[ index , 'NAME']:\n",
    "        original_df.loc[index, 'potential_youth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # cross sell success\n",
    "# original_df[(original_df['CROSS_SELL_SUCCESS']==1) & (original_df['potential_youth']==0)]['CROSS_SELL_SUCCESS'].count()/original_df[original_df['potential_youth']==0]['CROSS_SELL_SUCCESS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Cross Sell Results</center>\n",
    "\n",
    "\n",
    "Youth | % Success | % Failures |\n",
    "---|----|---|\n",
    "Potential Youth | 72% | 28% |\n",
    "Not Youth |  68% |32% |\n",
    "\n",
    "Not much difference in Cross Sell success conversion. This is an interesting insights, since we could assume that if a user is a youth they would be purchasing the wine. Unless they are a son or daughter of another user, but meet the requirement for age. Given that the youth feature we developed have a 72% success rate we can look into this further and monitor if it will have a significant impact on the model performance.\n",
    "\n",
    "\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature - Family Name\n",
    "Below we can explore the number of family members there are per name. When we explored the dataset initial we could see there were a number of members that had same names so we can expect a variety of large families.\n",
    "\n",
    "Family Name | Count |\n",
    "---|---|\n",
    "Frey         | 79\n",
    "Stark        | 32\n",
    "Lannister    | 29\n",
    "Tyrell       | 28\n",
    "Targaryen    | 24\n",
    "Targaryen    | 17\n",
    "Greyjoy      | 16\n",
    "Hightower    | 16\n",
    "Florent      | 14\n",
    "Vance        | 14\n",
    "\n",
    "Exploring the different family names, the name \"Frey\" appears very frequently a total of 79 times, which is almost twice the amount of the second name \"Stark\".\n",
    "\n",
    "We can develop a binary feature to determine whether or not a user is part of that family, and monitor it's influence on the success of cross selling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Most common family name - making them binary\n",
    "original_df['FAMILY_NAME'] = original_df['FAMILY_NAME'].astype(str)\n",
    "\n",
    "#column with all 0s\n",
    "original_df['frey_family'] = 0\n",
    "\n",
    "#loop to calissify frey family\n",
    "for index, val in original_df.iterrows():\n",
    "    if 'Frey' in original_df.loc[ index , 'FAMILY_NAME']:\n",
    "        original_df.loc[index, 'frey_family'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Family Name Results</center>\n",
    "\n",
    "\n",
    "Name | Count |\n",
    "---|----|\n",
    "Frey Family | 92  |\n",
    "Not Frey | 1854 |\n",
    "\n",
    "Interesting to see that even though our original dataset found only 79 Frey family members but when we developed this function to find all family names with  \"Frey\"  the results outputted 92 users. The Frey family name provides a decent sized sample to create a categorical binary feature.\n",
    "\n",
    "We will explore the success of the cross sell with the Frey family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # cross sell success\n",
    "# original_df[(original_df['CROSS_SELL_SUCCESS']==1) & (original_df['frey_family']!=1)]['CROSS_SELL_SUCCESS'].count()/original_df[original_df['frey_family']!=1]['CROSS_SELL_SUCCESS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Cross Sell Results</center>\n",
    "\n",
    "\n",
    "DOMAIN | % Success | % Failures |\n",
    "---|----|---|\n",
    "Frey Family | 66% | 34% |\n",
    "Not Frey |  68% |32% |\n",
    "\n",
    "Also looking at the cross sell success of the Frey name with Wine purchases there is no significant difference in conversion, since both are around 66-68% success rates.\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature - Number of Names\n",
    "There are multiple users that have extended names in both their full name and their family name. It will be interesting to split up and count the number of names in a single name. Below we will split the names by using a custom built strong function to get a count of the names available in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# function to split names\n",
    "#########################\n",
    "def text_split_feature(col, df, sep=' ', new_col_name='number_of_names'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Name\n",
    "We will first split the full name and see how many users have longer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# number of names in full name\n",
    "text_split_feature(col = 'NAME',\n",
    "                   df  = original_df,\n",
    "                   new_col_name = 'NUM_OF_NAMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # cross sell success\n",
    "# original_df[(original_df['CROSS_SELL_SUCCESS']==1) & (original_df['NUM_OF_NAMES']==6)]['CROSS_SELL_SUCCESS'].count()/original_df[original_df['NUM_OF_NAMES']==6]['CROSS_SELL_SUCCESS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Full Name Length Count </center>\n",
    "\n",
    "Number of Names | Count |\n",
    "--|--|\n",
    "2  |  1201|\n",
    "1   |  591|\n",
    "3   |   98|\n",
    "5   |   35|\n",
    "6   |   12|\n",
    "4    |   9|\n",
    "\n",
    "We can see majority of users have the 2 names, most commonly a first and last name. Interestingly we can see a decent amount of users have 3 names.\n",
    "\n",
    "Below let's explore the cross sell success based on name count.\n",
    "\n",
    "### <center>Cross Sell Results</center>\n",
    "\n",
    "\n",
    "DOMAIN | % Success | % Failures |\n",
    "---|----|---|\n",
    "1 Name | 51% | 49% |\n",
    "2 Names |  76% |24% |\n",
    "3 Names| 71% | 29% |\n",
    "4 Names| 56% | 44% |\n",
    "5 Names| 69% | 31% |\n",
    "6 Names| 83% | 17% |\n",
    "\n",
    "We can see that our 2nd most popular same length frequency count is 2, and it has 76% cross sell success. This can prove to be valuable insights later as we build the model since it is telling us potentially people will short full names tend to buy more wine. In comparison people who have 1 name in their full name would mean they do not have a last name and have low success in converting into a wine customer.\n",
    "\n",
    "### Family Name\n",
    "As well as the full name, we want to see if any customers have long last names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# number of family names\n",
    "text_split_feature(col = 'FAMILY_NAME',\n",
    "                   df  = original_df,\n",
    "                   new_col_name = 'NUM_OF_FAMILY_NAMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # cross sell success\n",
    "# original_df[(original_df['CROSS_SELL_SUCCESS']==1) & (original_df['NUM_OF_FAMILY_NAMES']==4)]['CROSS_SELL_SUCCESS'].count()/original_df[original_df['NUM_OF_FAMILY_NAMES']==4]['CROSS_SELL_SUCCESS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Family Name Length Count </center>\n",
    "Number of Family Names | Count |\n",
    "--|--|\n",
    "1  |  1800|\n",
    "2   |  134|\n",
    "3   |    7|\n",
    "4   |    5|\n",
    "\n",
    "We would expect to see customers with a single family name, but there are a number of users have last names at leats 2 names long. This potentially could add value later on in our model's performance.\n",
    "\n",
    "### <center>Cross Sell Results</center>\n",
    "\n",
    "\n",
    "DOMAIN | % Success | % Failures |\n",
    "---|----|---|\n",
    "1 Name | 68% | 32% |\n",
    "2 Names |  73% |27% |\n",
    "3 Names| 43% | 57% |\n",
    "4 Names| 60% | 40% |\n",
    "\n",
    "We can see that customer with 1 or 2 long family names have relatively high conversion for cross sell success. Users with 4 names also have a decent success rate, however there is only a sample size of 5 users so that would not be sufficient to validate that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "\n",
    "## Feature - Discount Plan\n",
    "The company states that a customer who meet the requirement below they will be eligible to receive a discount.\n",
    "\n",
    "\n",
    "Weekly Meal Plan | Total Meals| Discount |\n",
    "----|--| --|\n",
    "Basic | 3 |10% |\n",
    "Premium | 5 |20%|\n",
    "\n",
    "Below we will engineer features based on two conditions:\n",
    "\n",
    "    1. Must have a Weekly subscription which means at least a 1\n",
    "    2. Either have 3 or more total meals ordered to be eligible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# engineer discount\n",
    "discount_plan = []\n",
    "\n",
    "for index, col in original_df.iterrows():\n",
    "    \n",
    "    if (original_df.loc[index, 'WEEKLY_PLAN'] > 1) and (original_df.loc[index, 'TOTAL_MEALS_ORDERED'] == 3 or original_df.loc[index, 'TOTAL_MEALS_ORDERED'] == 4):\n",
    "        discount_plan.append('basic_discount')\n",
    "    elif (original_df.loc[index, 'WEEKLY_PLAN'] > 1) and (original_df.loc[index, 'TOTAL_MEALS_ORDERED'] > 4):\n",
    "        discount_plan.append('premium_discount')\n",
    "    else:\n",
    "        discount_plan.append('no_discount')\n",
    "\n",
    "discount_df = pd.DataFrame(discount_plan)\n",
    "\n",
    "discount_df.columns = ['discount']\n",
    "discount_df.head()\n",
    "\n",
    "\n",
    "original_df = pd.concat([original_df, discount_df], axis = 1)\n",
    "\n",
    "\n",
    "one_hot_discount = pd.get_dummies(original_df['discount'])\n",
    "\n",
    "original_df = original_df.join([one_hot_discount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_df['discount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount Plan | Count |\n",
    "----|--|\n",
    "Basic | 0 |\n",
    "Premium | 1400 |\n",
    "No Discount | 546 |\n",
    "\n",
    "\n",
    "- Looking at the results, there are no customers who meet the requirements of for BASIC discount.\n",
    "- We only have values for either premium or no discount eligibility.\n",
    "- There could be a relationship with customers who had the premium discount with their cross sell success so we can check that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # query to find the count of users whohad a positive cross sell and a discount type\n",
    "# original_df[(original_df['CROSS_SELL_SUCCESS']==1) & (original_df['discount']=='no_discount')]['CROSS_SELL_SUCCESS'].count()/original_df[original_df['discount']=='no_discount']['CROSS_SELL_SUCCESS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Cross Sell Results</center>\n",
    "\n",
    "\n",
    "DOMAIN | % Success | % Failures |\n",
    "---|----|---|\n",
    "Premium | 68% | 32% |\n",
    "Basic |  none |none |\n",
    "No Discount | 68% | 32% |\n",
    "\n",
    "\n",
    "We can see that a total of Premium discount and No Discount users had the same distribution in cross sell success, so this might not provide that much value as we go deeper into our models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature - Email Grouping\n",
    "The dataset provides us with customer emails, and the types of emails per group. Within the case it states that there are 3 email groups.\n",
    "\n",
    "    1. personal\n",
    "    2. professional\n",
    "    3. junk\n",
    "    \n",
    "To isolate whether or not an email is in this group we will need to split each email from the address and domain name. The goal is to have a single column that categorizes the emails by personal, professional and junk. To obtain this we will need to do the following:\n",
    "\n",
    "    1. split email address\n",
    "    2. group and classify email type\n",
    "    3. create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create email dummy variables\n",
    "\n",
    "# Splitting emails\n",
    "email_list = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in original_df.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    email_split = original_df.loc[index, 'EMAIL'].split(sep = \"@\")\n",
    "    \n",
    "    # add for loop results to list\n",
    "    email_list.append(email_split)\n",
    "    \n",
    "# converting into a DataFrame \n",
    "email_df = pd.DataFrame(email_list)\n",
    "\n",
    "# renaming columns\n",
    "email_df.columns = ['name' , 'EMAIL_DOMAIN']\n",
    "\n",
    "# concat email to df\n",
    "original_df = pd.concat([original_df, email_df['EMAIL_DOMAIN']],\n",
    "                   axis = 1)\n",
    "\n",
    "# email domains\n",
    "personal_emails = [ 'gmail.com','protonmail.com', 'yahoo.com', 'msn.com']\n",
    " \n",
    "professional_emails = [ 'amex.com','merck.com', 'mcdonalds.com','cocacola.com','jnj.com','nike.com', 'apple.com',            \n",
    "                        'ibm.com', 'ge.org','dupont.com','microsoft.com','chevron.com','unitedhealth.com',\n",
    "                        'travelers.com', 'exxon.com','boeing.com','verizon.com','mmm.com','pg.com','caterpillar.com',\n",
    "                        'disney.com','walmart.com','visa.com','pfizer.com','jpmorgan.com', 'unitedtech.com',\n",
    "                       'cisco.com','goldmansacs.com','intel.com','homedepot.com']   \n",
    "\n",
    "jumk_emails = ['me.com', 'aol.com','hotmail.com', 'live.com', 'msn.com','passport.com' ]\n",
    "\n",
    "\n",
    "domain_list = []\n",
    "\n",
    "for domain in original_df['EMAIL_DOMAIN']:\n",
    "    \n",
    "    # crete lists for personal\n",
    "    if domain in personal_emails:\n",
    "        domain_list.append(\"personal\") # categorical list\n",
    "        \n",
    "    elif domain in professional_emails:\n",
    "        domain_list.append(\"professional\") # categorical list\n",
    "\n",
    "    elif domain in jumk_emails:\n",
    "        domain_list.append(\"junk\") # categorical list    \n",
    "        \n",
    "    else:\n",
    "        print(domain)\n",
    "\n",
    "# create new series for domain type\n",
    "original_df['DOMAIN_TYPE'] = pd.Series(domain_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# original_df['DOMAIN_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Domain Count</center>\n",
    "\n",
    "\n",
    "DOMAIN | Count |\n",
    "---|----|\n",
    "Junk | 317 | \n",
    "Personal |  933 |\n",
    "Professional | 696 | \n",
    "\n",
    "More people are using their personal emails when creating an account with the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query to find the count of users who had a positive cross sell and their domain type\n",
    "# original_df[(original_df['CROSS_SELL_SUCCESS']==1) & (original_df['DOMAIN_TYPE']==\"professional\")]['CROSS_SELL_SUCCESS'].count()/original_df[original_df['DOMAIN_TYPE']==\"professional\"]['CROSS_SELL_SUCCESS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how the effects of email grouping responds to CROSS SELL SUCCESS. Below are the percentages based on each groups proportions.\n",
    "\n",
    "### <center>Cross Sell Results</center>\n",
    "\n",
    "\n",
    "DOMAIN | % Success | % Failures |\n",
    "---|----|---|\n",
    "Junk | 42% | 58% |\n",
    "Personal |  68% |32% |\n",
    "Professional | 80% | 20% |\n",
    "\n",
    "\n",
    "We can see that users who have professional domains have a pretty high successful conversion. This could mean that they are working and have more income which could encourage them to spend more in buying the wine, or are older and meet the age requirement for drinking wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# one hot emails\n",
    "one_hot_email             = pd.get_dummies(original_df['DOMAIN_TYPE'])\n",
    "\n",
    "# joining codings together\n",
    "original_df = original_df.join([one_hot_email],\n",
    "              sort=False)\n",
    "\n",
    "original_df = original_df.drop(['EMAIL','EMAIL_DOMAIN', 'DOMAIN_TYPE'],\n",
    "             axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "\n",
    "## Feature - Median Rating\n",
    "Given that median rating is on a set scale from 1 to 5, we can use this to create 5 dummy variables to expand our feature offerings. We can treat the median rating a categorical. Creating 5 groups based on the following:\n",
    "\n",
    "    1. 1 = v_low_rating\n",
    "    2. 2 = low_rating\n",
    "    3. 3 = medium_rating\n",
    "    4. 4 = high_rating\n",
    "    5. 5 = v_high_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# one hot encode rating\n",
    "original_df['RATING'] = 0\n",
    "\n",
    "for index, val in original_df.iterrows():\n",
    "    if original_df.loc[index, 'MEDIAN_MEAL_RATING'] == 1:\n",
    "        original_df.loc[index, 'RATING'] = 'v_low_rating'\n",
    "    elif original_df.loc[index, 'MEDIAN_MEAL_RATING'] == 2:\n",
    "        original_df.loc[index, 'RATING'] = 'low_rating'\n",
    "    elif original_df.loc[index, 'MEDIAN_MEAL_RATING'] == 3:\n",
    "        original_df.loc[index, 'RATING'] = 'medium_rating'        \n",
    "    elif original_df.loc[index, 'MEDIAN_MEAL_RATING'] == 4:\n",
    "        original_df.loc[index, 'RATING'] = 'high_rating'\n",
    "    elif original_df.loc[index, 'MEDIAN_MEAL_RATING'] == 5:\n",
    "        original_df.loc[index, 'RATING'] = 'v_high_rating'\n",
    "    else:\n",
    "        print('error')\n",
    "        \n",
    "original_df['RATING'].value_counts()\n",
    "\n",
    "one_hot_rating = pd.get_dummies(original_df['RATING'])\n",
    "\n",
    "original_df = original_df.join([one_hot_rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# original_df['RATING'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Rating Count </center>\n",
    "\n",
    "Rating | Count |\n",
    "---|----|\n",
    "Very Low Rating  (1)  |  99|\n",
    "Low Rating (2) |  487|\n",
    "Medium Rating (3) | 1087 |\n",
    "High Rating (4)  |  260|\n",
    "Very High Rating (5)  | 13|\n",
    "\n",
    "It looks like that a lot of our customer rated their meals  in the middle at 3 and more customer rated their meals lower compared to higher. This can provide insights to their disappointment on their experience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query to find the count of users who had a positive cross sell and their domain type\n",
    "# original_df[(original_df['CROSS_SELL_SUCCESS']==1) & (original_df['RATING']==\"v_high_rating\")]['CROSS_SELL_SUCCESS'].count()/original_df[original_df['RATING']==\"v_high_rating\"]['CROSS_SELL_SUCCESS'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Cross Sell Results</center>\n",
    "\n",
    "Rating | % Success | % Failures |\n",
    "---|----|---|\n",
    "Very Low Rating  (1)  | 65% | 35% |\n",
    "Low Rating (2) |  67% |33% |\n",
    "Medium Rating (3) | 68% | 32% |\n",
    "High Rating (4)  | 70% | 30% |\n",
    "Very High Rating (5)  |  85% |15% |\n",
    "\n",
    "Looking at how the ratings related to the cross sell success we see a steady growth as ratings increase in scores the percentage of positive wine purchases also increase. This could result in  user enjoying their meals happily adding on the additional wine purchase to their plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "# 3. Dataset Preparation\n",
    "Now that we have all the feature cleaned, below we want to make sure we remove the remaining columns that we will not need since we developed dummy variables or used them in another way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#drop categorical values\n",
    "original_df = original_df.drop(['NAME', 'FIRST_NAME', 'FAMILY_NAME',\n",
    "                               'MEDIAN_MEAL_RATING','RATING', 'discount'],\n",
    "                               axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Overview\n",
    "Below explores the correlation between our response variable - Cross Sell Success and our updated dataset with our new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# corr_original = original_df.corr().round(2)\n",
    "\n",
    "# #corr_original['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Top Correlation Results</center>\n",
    "\n",
    "Variable | Correlation\n",
    "----|---|\n",
    "FOLLOWED_RECOMMENDATIONS_PCT    |   0.46|\n",
    "professional                     |  0.19|\n",
    "NUM_OF_NAMES                     |  0.16|\n",
    "CANCELLATIONS_BEFORE_NOON        |  0.16|\n",
    "TOTAL_CANCELLATIONS               | 0.14|\n",
    "MOBILE_NUMBER                     | 0.10|\n",
    "junk                              |-0.25|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top results for positive and negative correlation, we see 3 features that we engineered rank among high correlations.\n",
    "        \n",
    "        - professional -> this relates to the WORK email domain\n",
    "        - NUM_OF_NAME -> the count of the number of names in a user has.\n",
    "        - junk -> relates to the other email domain type outside personal.\n",
    "        \n",
    "Seeing the value in the correlation we can proceed to the next steps in identifying the model and performance. It is interesting to see that some of the feature we developed are not appearing in top correlation when they should strong results when looking at the individually.\n",
    "\n",
    "Building the model will help determine whether or not these values are significant to our performance.\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variable Selection\n",
    "Now that we have established all the features we have built we can use various techniques to determine which features are most value to our model performance. In the following steps we will use two techniques to obtain the optimal features for variable selection. Once we obtain the trimmed down variables we can test it on multiple models to find which performance best based on their unique criteria.\n",
    "\n",
    "    - Stats Model -> removing features based on p-value \n",
    "    - Pruned Classification Tree -> studying a trimmed down CART model to find which values are truly significant\n",
    "\n",
    "## Stats Model\n",
    "In this step we will build a model using the statsmodel package. The goal of this is to identify the variables with the highest p-values so we can manually remove them and help optimize the model. If we don't do this step, and we look to build models using scikit learn later, redundant variables will be kept and will slow the model down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # define response and explanatory variables\n",
    "\n",
    "# #explanatory variable - eveything but Cross Sell\n",
    "# customer_data = original_df.drop('CROSS_SELL_SUCCESS', axis =1)\n",
    "\n",
    "# #response variable\n",
    "# customer_target = original_df['CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "When building the train and test split to be used in this dataset we must ensure the random_state remain the same so we can monitor the performance of the model over and over again without using altered split datasets. We will also split the dataset consistently 75% training and 25% testing.\n",
    "\n",
    "As mentioned earlier in the report we want to pass in the argument stratify to ensure, as well as the split we are getting an equal representation of our response variable success and failures. If we do not add this step we could have a split dataset with one side full of only successes which will not be a good representation of our overall data.\n",
    "\n",
    "<i>Stratification means to sort data/people/objects into distinct groups or layers</i> - [Statistics How to](https://www.statisticshowto.datasciencecentral.com/stratification-definition/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # import necessary package\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # train and test split on variables\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#                                             customer_data, \n",
    "#                                             customer_target,\n",
    "#                                             test_size = 0.25,\n",
    "#                                             random_state = 222,\n",
    "#                                             stratify = customer_target)\n",
    "\n",
    "# # merging training data for statsmodels\n",
    "# customer_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stats Model - Logistic Regression\n",
    "To get an overview of the model and check which variables are significant we will run the model against all potential explanatory variables. This will help flag all variables with high p-values that we could remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # getting explanatory variables while remopving addtional onehot encoded extra variable\n",
    "# for val in customer_data:\n",
    "#     print(f\"{val} +\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # instantiating a logistic regression model object\n",
    "# logistic_full = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~\n",
    "#                                         REVENUE +\n",
    "#                                         TOTAL_MEALS_ORDERED +\n",
    "#                                         UNIQUE_MEALS_PURCH +\n",
    "#                                         CONTACTS_W_CUSTOMER_SERVICE +\n",
    "#                                         PRODUCT_CATEGORIES_VIEWED +\n",
    "#                                         AVG_TIME_PER_SITE_VISIT +\n",
    "#                                         MOBILE_NUMBER +\n",
    "#                                         CANCELLATIONS_BEFORE_NOON +\n",
    "#                                         CANCELLATIONS_AFTER_NOON +\n",
    "#                                         TASTES_AND_PREFERENCES +\n",
    "#                                         PC_LOGINS +\n",
    "#                                         MOBILE_LOGINS +\n",
    "#                                         WEEKLY_PLAN +\n",
    "#                                         EARLY_DELIVERIES +\n",
    "#                                         LATE_DELIVERIES +\n",
    "#                                         PACKAGE_LOCKER +\n",
    "#                                         REFRIGERATED_LOCKER +\n",
    "#                                         FOLLOWED_RECOMMENDATIONS_PCT +\n",
    "#                                         AVG_PREP_VID_TIME +\n",
    "#                                         LARGEST_ORDER_SIZE +\n",
    "#                                         MASTER_CLASSES_ATTENDED +\n",
    "#                                         AVG_CLICKS_PER_VISIT +\n",
    "#                                         TOTAL_PHOTOS_VIEWED +\n",
    "#                                         log_TOTAL_MEALS_ORDERED +\n",
    "#                                         log_UNIQUE_MEALS_PURCH +\n",
    "#                                         log_CONTACTS_W_CUSTOMER_SERVICE +\n",
    "#                                         log_PRODUCT_CATEGORIES_VIEWED +\n",
    "#                                         log_AVG_TIME_PER_SITE_VISIT +\n",
    "#                                         log_PC_LOGINS +\n",
    "#                                         log_AVG_PREP_VID_TIME +\n",
    "#                                         log_MEDIAN_MEAL_RATING +\n",
    "#                                         log_AVG_CLICKS_PER_VISIT +\n",
    "#                                         out_TOTAL_MEALS_ORDERED +\n",
    "#                                         out_UNIQUE_MEALS_PURCH +\n",
    "#                                         out_CONTACTS_W_CUSTOMER_SERVICE +\n",
    "#                                         out_PRODUCT_CATEGORIES_VIEWED +\n",
    "#                                         out_AVG_TIME_PER_SITE_VISIT +\n",
    "#                                         out_CANCELLATIONS_BEFORE_NOON +\n",
    "#                                         out_CANCELLATIONS_AFTER_NOON +\n",
    "#                                         out_PC_LOGINS +\n",
    "#                                         out_WEEKLY_PLAN +\n",
    "#                                         out_EARLY_DELIVERIES +\n",
    "#                                         out_LATE_DELIVERIES +\n",
    "#                                         out_AVG_PREP_VID_TIME +\n",
    "#                                         out_LARGEST_ORDER_SIZE +\n",
    "#                                         out_MASTER_CLASSES_ATTENDED +\n",
    "#                                         out_MEDIAN_MEAL_RATING +\n",
    "#                                         out_AVG_CLICKS_PER_VISIT +\n",
    "#                                         out_TOTAL_PHOTOS_VIEWED +\n",
    "#                                         tre_TOTAL_MEALS_ORDERED +\n",
    "#                                         tre_CANCELLATIONS_AFTER_NOON +\n",
    "#                                         tre_WEEKLY_PLAN +\n",
    "#                                         tre_LATE_DELIVERIES +\n",
    "#                                         tre_AVG_PREP_VID_TIME +\n",
    "#                                         tre_MASTER_CLASSES_ATTENDED +\n",
    "#                                         tre_MEDIAN_MEAL_RATING +\n",
    "#                                         tre_TOTAL_PHOTOS_VIEWED +\n",
    "#                                         potential_youth +\n",
    "#                                         frey_family +\n",
    "#                                         NUM_OF_NAMES +\n",
    "#                                         NUM_OF_FAMILY_NAMES +\n",
    "#                                         junk +\n",
    "#                                         professional +\n",
    "#                                         high_rating +\n",
    "#                                         v_high_rating\n",
    "#                                         \"\"\",\n",
    "#                                         data    = customer_train)\n",
    "\n",
    "\n",
    "# # fitting the model object\n",
    "# results_full = logistic_full.fit()\n",
    "\n",
    "\n",
    "# # checking the results SUMMARY\n",
    "# results_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Logistic Regression Overview </center>\n",
    "#### <center>(Portion of Results) </center>\n",
    "\n",
    "Dep. Variable:|\tCROSS_SELL_SUCCESS\t|No. Observations:|\t1459|\n",
    "----|---|---|---|\n",
    "Model:|\tLogit|\tDf Residuals:|\t1395|\n",
    "Method:|\tMLE|\tDf Model:|\t63|\n",
    "Date:|\tWed, 11 Mar 2020|\tPseudo R-squ.:\t|0.3830|\n",
    "Time:|\t22:12:55\t|Log-Likelihood:|\t-565.25|\n",
    "converged:|\tTrue\t|LL-Null:\t|-916.19|\n",
    "Covariance Type:\t|nonrobust\t|LLR p-value:\t|1.229e-108|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "coef\t|std err\t|z|\tP>z|\t0.025|\t0.975|\n",
    "--------|-----------|-|-|-|-|\n",
    "Intercept\t|-39.6902|\t1.7e+06\t|-2.33e-05|\t1.000|\t-3.33e+06|3.33e+06|\n",
    "REVENUE|\t-0.0004|\t0.000\t|-3.198|\t0.001\t|-0.001|\t-0.000|\n",
    "TOTAL_MEALS_ORDERED|\t-0.0053\t|0.007\t|-0.751|\t0.453|\t-0.019|\t0.009|\n",
    "UNIQUE_MEALS_PURCH\t|0.4016|\t0.134|\t3.007|\t0.003\t|0.140|\t0.663|\n",
    "CONTACTS_W_CUSTOMER_SERVICE|\t-0.0137|\t0.231|\t-0.059|\t0.953|\t-0.467|\t0.439|\n",
    "PRODUCT_CATEGORIES_VIEWED|\t0.0584\t|0.204\t|0.286\t|0.775\t|-0.342|\t0.459|\n",
    "AVG_TIME_PER_SITE_VISIT\t|0.0050\t|0.004\t|1.257\t|0.209|\t-0.003\t|0.013|\n",
    "MOBILE_NUMBER|\t0.9130|\t0.227|\t4.015\t|0.000\t|0.467\t|1.359|\n",
    "CANCELLATIONS_BEFORE_NOON|\t0.4007|\t0.077|\t5.232\t|0.000|\t0.251|\t0.551|\n",
    "CANCELLATIONS_AFTER_NOON\t|-0.4835|\t0.455|\t-1.063|\t0.288|\t-1.375|\t0.408|\n",
    "\n",
    "\n",
    "These results only represent the first few lines of the model.Looking at the performance, we can see we have a lot of variables that have a p-value higher than 0.05. In general we want to reject variables that fall above this threshold. Lower than or equal to 0.05 mean's the variables are statistically significant for our model's performance and if we were to remove one we could significantly alter the results. However, we cannot base that if we do remove all p-value that are higher than 0.05 our model will be sound. Removing the high p-values would only give us addtional support to improve and continue to develop the model (Source: [Mcleod (2019) Simply Psychology](https://www.simplypsychology.org/p-value.html))\n",
    "\n",
    "Along with the p-values we see there are numerous coefficients with negative values. The coefficient tells us that for every unit increase in that variable the change will follow suit. \n",
    "\n",
    "For example we see 'UNIQUE_MEALS_PURCH' has a 0.4016 coefficient, we can determine that if there is a change in positive 1 in the UNIQUE_MEALS_PURCH the overall value chances of CROSS_SELL_SUCCESS will increase by 0.4016, therefore having a positive impact on the response variable. (Source: [Princeton](https://dss.princeton.edu/online_help/analysis/interpreting_regression.htm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Stats Logistic Model - Removing p-values\n",
    "Below we will remove the flagged p-values one by one if they have a higher than 0.05 value, and run the model again. Our goal is to narrow down the features. The reason we remove one p-value at a time is because if we decided to remove all the high values simultaneously it may effect other variables. So we must remove one by one, then observe the changes, then proceed to remove - this ensures our results remain in tack and will not cause any skewed performance. (Source: [Minitab](https://blog.minitab.com/blog/understanding-statistics/three-common-p-value-mistakes-youll-never-have-to-make))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# # instantiating a logistic regression model object\n",
    "# logistic_full = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~\n",
    "#                                         MOBILE_NUMBER +\n",
    "#                                         CANCELLATIONS_BEFORE_NOON +\n",
    "#                                         CANCELLATIONS_AFTER_NOON +\n",
    "#                                         TASTES_AND_PREFERENCES +\n",
    "#                                         MOBILE_LOGINS +\n",
    "#                                         FOLLOWED_RECOMMENDATIONS_PCT +\n",
    "#                                         log_TOTAL_MEALS_ORDERED +\n",
    "#                                         out_TOTAL_MEALS_ORDERED +\n",
    "#                                         out_CONTACTS_W_CUSTOMER_SERVICE +\n",
    "#                                         out_CANCELLATIONS_BEFORE_NOON +\n",
    "#                                         out_AVG_PREP_VID_TIME +\n",
    "#                                         out_TOTAL_PHOTOS_VIEWED +\n",
    "#                                         potential_youth +\n",
    "#                                         NUM_OF_NAMES +\n",
    "#                                         NUM_OF_FAMILY_NAMES +\n",
    "#                                         junk +\n",
    "#                                         professional\n",
    "#                                         \"\"\",\n",
    "#                                         data    = customer_train)\n",
    "\n",
    "# # fitting the model object\n",
    "# results_full = logistic_full.fit()\n",
    "\n",
    "# # checking the results SUMMARY\n",
    "# results_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dep. Variable:|\tCROSS_SELL_SUCCESS\t|No. Observations:|\t1459|\n",
    "---|---|---|----|\n",
    "Model:|\tLogit\t|Df Residuals:\t|1441|\n",
    "Method:|\tMLE\t|Df Model:|\t17|\n",
    "Date:|\tThu, 12 Mar 2020|\tPseudo R-squ.:\t|0.3522|\n",
    "Time:|\t21:54:58\t|Log-Likelihood:|\t-593.46|\n",
    "converged:|\tTrue|\tLL-Null:\t|-916.19|\n",
    "Covariance Type:\t|nonrobust|\tLLR p-value:|\t3.312e-126|\n",
    "\n",
    "\n",
    "coef\t|std err|\tz\t|P>z|\t0.025|\t0.975|\n",
    "----|----|-----|----|----|----|\n",
    "Intercept|\t-4.3406|\t0.817|\t-5.314\t|0.000\t|-5.941\t|-2.740|\n",
    "MOBILE_NUMBER|\t0.7923|\t0.213\t|3.714\t|0.000\t|0.374|\t1.210|\n",
    "CANCELLATIONS_BEFORE_NOON|\t0.3886|\t0.074|\t5.247\t|0.000|\t0.243\t|0.534|\n",
    "CANCELLATIONS_AFTER_NOON|\t-0.3310|\t0.159\t|-2.083|\t0.037|\t-0.643|\t-0.020|\n",
    "TASTES_AND_PREFERENCES\t|0.3783|\t0.160|\t2.371\t|0.018|\t0.066|\t0.691|\n",
    "MOBILE_LOGINS|\t-0.2688\t|0.136|\t-1.977\t|0.048\t|-0.535|\t-0.002|\n",
    "FOLLOWED_RECOMMENDATIONS_PCT|\t0.0585|\t0.004|\t14.049|\t0.000\t|0.050\t|0.067|\n",
    "log_TOTAL_MEALS_ORDERED|\t0.5196|\t0.162|\t3.198\t|0.001|\t0.201|\t0.838|\n",
    "out_TOTAL_MEALS_ORDERED\t|-0.9791|\t0.335\t|-2.920|\t0.003|\t-1.636|\t-0.322|\n",
    "out_CONTACTS_W_CUSTOMER_SERVICE\t|0.6322|\t0.281\t|2.247|\t0.025|\t0.081|\t1.184|\n",
    "out_CANCELLATIONS_BEFORE_NOON|\t-1.1228|\t0.387|\t-2.902|\t0.004\t|-1.881\t|-0.364|\n",
    "out_AVG_PREP_VID_TIME|\t0.9568|\t0.473|\t2.022\t|0.043|\t0.030|\t1.884|\n",
    "out_TOTAL_PHOTOS_VIEWED\t|0.3522|\t0.158|\t2.232\t|0.026\t|0.043|\t0.661|\n",
    "potential_youth|\t-2.0547|\t0.570\t|-3.606\t|0.000|\t-3.171|\t-0.938|\n",
    "NUM_OF_NAMES\t|1.3273|\t0.151|\t8.809\t|0.000\t|1.032|\t1.623|\n",
    "NUM_OF_FAMILY_NAMES\t|-1.9351|\t0.321|\t-6.035|\t0.000|\t-2.564\t|-1.307|\n",
    "junk\t|-1.1875\t|0.199|\t-5.977\t|0.000\t|-1.577|\t-0.798|\n",
    "professional\t|0.8810|\t0.168\t|5.234|\t0.000\t|0.551|\t1.211|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model now has all the p-values that were higher than 0.05 removed. We can see these specific variables to run in our Scikit learn models and test out how they respond in terms of performance. Some interesting to note for removing the values.\n",
    "\n",
    "With these update variables for the feature selection we can observe the following:\n",
    "\n",
    "    - Very High Rating (5), had a very high CROSS SELL SUCCESS rate but is not significant to remain.\n",
    "    - professional had a high success rate and as expected remains in the features.\n",
    "    - Both the number of names in full and family names have remained as significant variables.\n",
    "    - potential youth despite having lower counts to classify youth, remained in the model due to significance.\n",
    "    - a handful of outlier features remained.\n",
    "    - no threshold features made it to the final model\n",
    "    - logarithm of total meals was the only log feature that remains in the model.\n",
    "    \n",
    "Overall we can see a total of 17 variables to be used in the future models, and 11 of them were engineered. Proving that the feature engineering stage to be vital in helping optimize the model performance.\n",
    "\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Model - Pruned Tree\n",
    "Using the decision tree model, we can prune the number of leafs we would like the model to output. We do this to ensure we are reducing the amount of leafs the model will run through and provide us with a result that is easier to comprehend. Pruning the tree will help provide us a list of which features the decision tree model believes to be most significant for the model performance.\n",
    "\n",
    "Once we obtain which features are deemed significant from the decision tree we can use this to build out a separate  variable list. This process will be used later on in the model and once we get the significant variables we can bring the variables back to this stage and use it on all the models.\n",
    "\n",
    "We will see later that the significant variables for the pruned tree will be.\n",
    "\n",
    "    - professional\n",
    "    - junk\n",
    "    - NUM_OF_NAMES\n",
    "    - FOLLOWED_RECOMMENDATIONS_PCT\n",
    "    - WEEKLY_PLAN\n",
    "    - CANCELLATIONS_BEFORE_NOON\n",
    "\n",
    "Similarly to the adjust p-value model, we have professional, junk, and number of names as the features we developed. However, there are significantly less features in this model. A total of 6 in comparison to the 17 we got from adjust the p-values.\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Testing - Scikit Learn\n",
    "\n",
    "Now that we have built the model remove p-values we can test out various models in Scikit learn. We have our set of explanatory variables we want to use. We can create a dictionary with just the variables we need so we can call it in later in each training and testing dataset.\n",
    "\n",
    "We will be testing out two types of explanatory variable selection:\n",
    "\n",
    "    1. Logistic Regression - Adjust p-values\n",
    "    2. Pruned Tree\n",
    "\n",
    "We will use the above explanatory variables selection on the following models in Scikit Learn's library;\n",
    "\n",
    "    1. Logistic Regression\n",
    "    2. K Nearest Neighbors Non Standardized\n",
    "    3. Classification Tree - Pruned\n",
    "    4. Support Vector Machine - SVM\n",
    "    5. Bayes\n",
    "    6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # user defined functions\n",
    "# ########################################\n",
    "# # optimal_neighbors\n",
    "# ########################################\n",
    "# def optimal_neighbors(X_data,\n",
    "#                       y_data,\n",
    "#                       standardize = True,\n",
    "#                       pct_test=0.25,\n",
    "#                       seed=802,\n",
    "#                       response_type='reg',\n",
    "#                       max_neighbors=20,\n",
    "#                       show_viz=True):\n",
    "#     \"\"\"\n",
    "# Exhaustively compute training and testing results for KNN across\n",
    "# [1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "# visualization of the results.\n",
    "# PARAMETERS\n",
    "# ----------\n",
    "# X_data        : explanatory variable data\n",
    "# y_data        : response variable\n",
    "# standardize   : whether or not to standardize the X data, default True\n",
    "# pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "# seed          : random seed to be used in algorithm, default 802\n",
    "# response_type : type of neighbors algorithm to use, default 'reg'\n",
    "#     Use 'reg' for regression (KNeighborsRegressor)\n",
    "#     Use 'class' for classification (KNeighborsClassifier)\n",
    "# max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "# show_viz      : display or surpress k-neigbors visualization, default True\n",
    "# \"\"\"    \n",
    "    \n",
    "    \n",
    "#     if standardize == True:\n",
    "#         # optionally standardizing X_data\n",
    "#         scaler             = StandardScaler()\n",
    "#         scaler.fit(X_data)\n",
    "#         X_scaled           = scaler.transform(X_data)\n",
    "#         X_scaled_df        = pd.DataFrame(X_scaled)\n",
    "#         X_data             = X_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "#     # train-test split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "#                                                         y_data,\n",
    "#                                                         test_size = pct_test,\n",
    "#                                                         random_state = seed)\n",
    "\n",
    "\n",
    "#     # creating lists for training set accuracy and test set accuracy\n",
    "#     training_accuracy = []\n",
    "#     test_accuracy = []\n",
    "    \n",
    "    \n",
    "#     # setting neighbor range\n",
    "#     neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "#     for n_neighbors in neighbors_settings:\n",
    "#         # building the model based on response variable type\n",
    "#         if response_type == 'reg':\n",
    "#             clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "#             clf.fit(X_train, y_train)\n",
    "            \n",
    "#         elif response_type == 'class':\n",
    "#             clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "#             clf.fit(X_train, y_train)            \n",
    "            \n",
    "#         else:\n",
    "#             print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "#         # recording the training set accuracy\n",
    "#         training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "#         # recording the generalization accuracy\n",
    "#         test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "#     # optionally displaying visualization\n",
    "#     if show_viz == True:\n",
    "#         # plotting the visualization\n",
    "#         fig, ax = plt.subplots(figsize=(12,8))\n",
    "#         plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "#         plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#         plt.xlabel(\"n_neighbors\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "    \n",
    "#     # returning optimal number of neighbors\n",
    "#     print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "#     return test_accuracy.index(max(test_accuracy))+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can create a function that helps visualize the confusion matrix for the model performance. This will help give a visual representation of the true positive, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# user defined function to visual confusion matrix\n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Selection\n",
    "Earlier we determined which features would be optimal for p-value removal and a pruned tree. Below we can create a dictionary so we can easily call and adjust the model for easier customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#dictionary\n",
    "candidate_dict = {\n",
    " \n",
    " # significant variables only from stats model\n",
    " 'logit_sig'    : ['MOBILE_NUMBER',\n",
    "                   'CANCELLATIONS_BEFORE_NOON',\n",
    "                   'CANCELLATIONS_AFTER_NOON',\n",
    "                   'TASTES_AND_PREFERENCES',\n",
    "                   'MOBILE_LOGINS',\n",
    "                   'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "                   'log_TOTAL_MEALS_ORDERED',\n",
    "                   'out_TOTAL_MEALS_ORDERED',\n",
    "                   'out_CONTACTS_W_CUSTOMER_SERVICE',\n",
    "                   'out_CANCELLATIONS_BEFORE_NOON',\n",
    "                   'out_AVG_PREP_VID_TIME',\n",
    "                   'out_TOTAL_PHOTOS_VIEWED',\n",
    "                   'potential_youth',\n",
    "                   'NUM_OF_NAMES',\n",
    "                   'NUM_OF_FAMILY_NAMES',\n",
    "                   'junk',\n",
    "                   'professional'\n",
    "                  ],\n",
    "  \n",
    "# significant values from pruned tree\n",
    " 'pruned_tree'  : ['professional',\n",
    "                    'junk',\n",
    "                    'NUM_OF_NAMES',\n",
    "                    'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "                    'WEEKLY_PLAN',\n",
    "                    'CANCELLATIONS_BEFORE_NOON']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure our response variable is CROSS_SELL_SUCCESS, and we have our newly created dictionary of significant variables indexed as our only explanatory variables to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "customer_data   =  original_df.loc[ : , candidate_dict['pruned_tree']]\n",
    "customer_target =  original_df['CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Metric\n",
    "After running our explanatory variables through each model we will obtain the ROC AUC score, not accuracy.\n",
    "\n",
    "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve.\n",
    "\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "    True Positive Rate\n",
    "    False Positive Rate\n",
    "\n",
    "Source: [Google Developers](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n",
    "\n",
    "<strong>Confusion Matrix</strong><br>\n",
    "The following confusion matrix can be read as follows:<br><br>\n",
    "\n",
    "~~~\n",
    "                   |\n",
    "  True Negatives   |  False Positives\n",
    "  (correct)        |  (incorrect)\n",
    "                   |\n",
    "-------------------|------------------\n",
    "                   |\n",
    "  False Negatives  |  True Positives\n",
    "  (incorrect)      |  (correct)\n",
    "                   |\n",
    "~~~\n",
    "\n",
    "<br><br>\n",
    "In terms of our model:<br><br>\n",
    "\n",
    "~~~\n",
    "\n",
    "TRUE NEGATIVE                                           |  FALSE POSITIVE\n",
    "PREDICTED: DID NOT GET WINE (CROSS_SELL_SUCCESS = 0)    |  PREDICTED: DID GET WINE (CROSS_SELL_SUCCESS = 1)\n",
    "ACTUAL:    DID NOT GET WINE (CROSS_SELL_SUCCESS = 0)    |  ACTUAL:    DID NOT GET WINE (CROSS_SELL_SUCCESS = 0)\n",
    "                                                        |\n",
    "--------------------------------------------------------|-------------------------------------------\n",
    "FALSE NEGATIVE                                          |  TRUE POSITIVE\n",
    "PREDICTED: DID NOT GET WINE (CROSS_SELL_SUCCESS = 0)    |  PREDICTED: DID GET WINE (CROSS_SELL_SUCCESS = 1)\n",
    "ACTUAL:    DID GET WINE     (CROSS_SELL_SUCCESS = 1)    |  ACTUAL:    DID GET WINE (CROSS_SELL_SUCCESS = 1)\n",
    "                                                        |  \n",
    "~~~\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## Logistic Regression\n",
    "To begin the model testing, we will use the logistic regression model. We did use this model earlier with the Stats Model library, however this will be used through Scikit learn's library. Logistic regression is a common practice to get an overview of binary classification in modeling. It helps predict whether or not a model is a 0 or 1, in this case our CROSS SELL SUCCESS on whether a customer is converting into a wine purchaser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # logistic regression\n",
    "\n",
    "# # train test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             customer_data,\n",
    "#             customer_target,\n",
    "#             test_size    = 0.25,\n",
    "#             random_state = 222,\n",
    "#             stratify     = customer_target)\n",
    "\n",
    "# # INSTANTIATING a logistic regression model\n",
    "# logreg = LogisticRegression(solver = 'lbfgs',\n",
    "#                             C = 1,\n",
    "#                             random_state = 222)\n",
    "\n",
    "\n",
    "# # FITTING the training data\n",
    "# logreg_fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # PREDICTING based on the testing set\n",
    "# logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# # SCORING the results\n",
    "# print('Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "# print('Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "# print('AUC  Score       :', roc_auc_score(y_true  = y_test, y_score = logreg_pred).round(4))\n",
    "\n",
    "\n",
    "\n",
    "# logreg_score_train = logreg_fit.score(X_train, y_train).round(4)\n",
    "# logreg_score_test  = logreg_fit.score(X_test, y_test).round(4)\n",
    "# logreg_AUC         = roc_auc_score(y_true  = y_test, y_score = logreg_pred).round(4)\n",
    "\n",
    "\n",
    "\n",
    "# # creating an empty list\n",
    "# model_performance = [['Model', 'Training Accuracy',\n",
    "#                       'Testing Accuracy', 'AUC Value']]\n",
    "\n",
    "# # saving the results\n",
    "# model_performance.append(['Logistic Regression',\n",
    "#                           logreg_score_train,\n",
    "#                           logreg_score_test,\n",
    "#                           logreg_AUC])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center> Logisitc Model AUC Score </center>\n",
    "\n",
    "Feature Selection | Score |\n",
    "---|-----|\n",
    "Logistic Regression Features | 0.7242\n",
    "Pruned Tree Features | 0.6961\n",
    "\n",
    "We see the adjust stats model with adjusted p-values scored higher. This could suggest that more features, as well as those specific features help the logistic model's performance since there is a noticeable 0.03 difference betweeen the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # creating a confusion matrix\n",
    "# print(confusion_matrix(y_true = y_test,\n",
    "#                        y_pred = logreg_pred))\n",
    "\n",
    "# # calling the visual_cm function\n",
    "# visual_cm(true_y = y_test,\n",
    "#           pred_y = logreg_pred,\n",
    "#           labels = ['Bought Wine', \"Didn't buy Wine\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "\n",
    "## KNN Non-Standardized\n",
    "KNN algorithms use data and classify new data points based on similarity measures (e.g. distance function). Classification is done by a majority vote to its neighbors. The data is assigned to the class which has the nearest neighbors. As you increase the number of nearest neighbors, the value of k, accuracy might increase. The goal of using KNN is to find clusters of groups that meet certain characteristics. Source: [Srivastava (2019) - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # train-test split with the nomral data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             customer_data,\n",
    "#             customer_target,\n",
    "#             random_state = 222,\n",
    "#             test_size = 0.25,\n",
    "#             stratify = customer_target)\n",
    "\n",
    "# # creating lists for training set accuracy and test set accuracy\n",
    "# training_accuracy = []\n",
    "# test_accuracy = []\n",
    "\n",
    "\n",
    "# # building a visualization of 1 to 50 neighbors\n",
    "# neighbors_settings = range(1, 21)\n",
    "\n",
    "\n",
    "# for n_neighbors in neighbors_settings:\n",
    "#     # Building the model\n",
    "#     clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "#     clf.fit(X_train, y_train)\n",
    "    \n",
    "#     # Recording the training set accuracy\n",
    "#     training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "#     # Recording the generalization accuracy\n",
    "#     test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# # plotting the visualization\n",
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "# plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "# plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.xlabel(\"n_neighbors\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # finding the optimal number of neighbors\n",
    "# non_stand_opt_neighbors = test_accuracy.index(max(test_accuracy)) + 1\n",
    "# print(f\"\"\"The optimal number of neighbors is {non_stand_opt_neighbors}\"\"\")\n",
    "\n",
    "\n",
    "# # INSTANTIATING a model with the optimal number of neighbors\n",
    "# knn_non_stand = KNeighborsClassifier(algorithm = 'auto',\n",
    "#                    n_neighbors = non_stand_opt_neighbors)\n",
    "\n",
    "\n",
    "# # FITTING the model based on the training data\n",
    "# knn_non_stand_fit = knn_non_stand.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # PREDITCING on new data\n",
    "# knn_non_stand_pred = knn_non_stand.predict(X_test)\n",
    "\n",
    "\n",
    "# # SCORING the results\n",
    "# print('Training Score   :', knn_non_stand.score(X_train, y_train).round(4))\n",
    "# print('Testing Score    :',  knn_non_stand.score(X_test, y_test).round(4))\n",
    "# print('AUC Score        :', roc_auc_score(y_true  = y_test, y_score = knn_non_stand_pred).round(4))\n",
    "# print('Optimal Neighbors:', non_stand_opt_neighbors)\n",
    "\n",
    "# # saving scoring data for future use\n",
    "# knn_non_stand_score_train = knn_non_stand_fit.score(X_train, y_train).round(4)\n",
    "# knn_non_stand_score_test  = knn_non_stand_fit.score(X_test, y_test).round(4)\n",
    "# knn_non_stand_AUC_score  = roc_auc_score(y_true  = y_test,\n",
    "#                                           y_score = knn_non_stand_pred).round(4)\n",
    "\n",
    "# # saving the results\n",
    "# model_performance.append(['KNN Non Standardized',\n",
    "#                           knn_non_stand_score_train,\n",
    "#                           knn_non_stand_score_test,\n",
    "#                           knn_non_stand_AUC_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center> KNN Non Standardized AUC Score </center>\n",
    "\n",
    "Feature Selection | Score |\n",
    "---|-----|\n",
    "Logistic Regression Features | 0.7557\n",
    "Pruned Tree Features | 0.7721\n",
    "\n",
    "We see the adjusted pruned tree model scored higher by 0.02. This provides us information that using the pruned tree explanatory variables KNN was able to produce stronger pattern recognition for the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "# # confusion matrix\n",
    "\n",
    "# # creating a confusion matrix\n",
    "# print(confusion_matrix(y_true = y_test,\n",
    "#                        y_pred = knn_non_stand_pred))\n",
    "\n",
    "# # calling the visual_cm function\n",
    "# visual_cm(true_y = y_test,\n",
    "#           pred_y = knn_non_stand_pred,\n",
    "#           labels = ['Bought Wine', \"Didn't buy Wine\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree - Pruned\n",
    "As mentioned earlier this is the stage where we try obtain the optimal features that was used in the models in previous steps. In a classification tree, the model breaks down each step into binary partitioning. This means, splitting a leaf (node) into two paths and determines whether or not the variable meets the requirements to continue in either direction. The goal is continuously split the tree and leafs by every new features the model outputs.\n",
    "\n",
    "Reducing the number of leafs in the model will allow easier recognition or business insights that can produce opportunities to take action. Decision trees historically over fit models if they are not tuned and pruned. (Source: [Towards Data Science](https://towardsdatascience.com/decision-tree-classification-de64fc4d5aac))\n",
    "\n",
    "Below we will develop a pruned tree model. However, the original analysis the tree was built using all the explanatory variables possible to determine which variables are most significant for all the models. The results below are the updated, trimmed explanatory variables in order to simplify the analysis and not over load irrelevant variables to the audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# function to graph features importance\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# libraries for classification trees\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import export_graphviz             # exports graphics\n",
    "from sklearn.externals.six import StringIO           # saves objects in memory\n",
    "from IPython.display import Image                    # displays on frontend\n",
    "import pydotplus                                     # interprets dot objects\n",
    "\n",
    "# train-test split with the nomral data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            customer_data,\n",
    "            customer_target,\n",
    "            random_state = 222,\n",
    "            test_size = 0.25,\n",
    "            stratify = customer_target)\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "tree_pruned = DecisionTreeClassifier(max_depth=4,\n",
    "                    min_samples_leaf = 25,\n",
    "                    random_state = 222)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "tree_pruned_fit = tree_pruned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "tree_pred = tree_pruned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# # SCORING the model\n",
    "# print('Training ACCURACY:', tree_pruned_fit.score(X_train, y_train).round(4))\n",
    "# print('Testing  ACCURACY:', tree_pruned_fit.score(X_test, y_test).round(4))\n",
    "# print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "#                                           y_score = tree_pred).round(4))\n",
    "\n",
    "\n",
    "# plotting feature importance\n",
    "plot_feature_importances(tree_pruned_fit,\n",
    "                         train = X_train,\n",
    "                         export = False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "<center> Pruned Tree AUC Score </center>\n",
    "\n",
    "Feature Selection | Score |\n",
    "---|-----|\n",
    "Logistic Regression Features | 0.7314\n",
    "Pruned Tree Features | 0.7162\n",
    "\n",
    "We see the adjust Logistic Regression Features scored higher. What is more important here is to identify the features that were significant. We use these features back in the top portion of the model to see if there is a impact on model performance.\n",
    "\n",
    "\n",
    "Originally we tested  all the explanatory variables on the pruned tree which gave us the insights to select the features in the graph above. We used those features to run on all the models, since it helps provide an additional insight to model performance.\n",
    "\n",
    "We see that the following features are of value to the tree.\n",
    "\n",
    "    - professional\n",
    "    - junk\n",
    "    - NUM_OF_NAMES\n",
    "    - FOLLOWED_RECOMMENDATIONS_PCT\n",
    "    - WEEKLY_PLAN\n",
    "    - CANCELLATIONS_BEFORE_NOON\n",
    "    \n",
    "    \n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Model - SVM\n",
    "SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. It is known for its kernel trick to handle nonlinear input spaces. \n",
    "\n",
    "Support vectors are the data points, which are closest to the divider known as the hyperplane of the dataset. \n",
    "\n",
    "A hyperplane finds a way to separates the data based on certain characteristics. The main goal is the develop a way to divide the dataset in the most efficient was possible, with the final output looking to measure the distance between the nearest points - known as the margin. Once the margin is identified,  the hyperplane that has the furthest margin between the data points in the dataset is selected.\n",
    "\n",
    "\n",
    "(Source: [Datacamp](https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python#svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7587\n",
      "Testing  ACCURACY: 0.7803\n",
      "AUC Score        : 0.818\n"
     ]
    }
   ],
   "source": [
    "# SVM model Kernal\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            customer_data,\n",
    "            customer_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 222,\n",
    "            stratify     = customer_target)\n",
    "\n",
    "#import package\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#instantiate model\n",
    "svc_kernal_model = SVC(kernel = 'rbf', random_state = 222)\n",
    "\n",
    "#fit model\n",
    "svc_kernal_fit = svc_kernal_model.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on new data\n",
    "svc_kernal_pred = svc_kernal_model.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', svc_kernal_model.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', svc_kernal_model.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = svc_kernal_pred).round(4))\n",
    "\n",
    "#save results\n",
    "svc_kernal_score_train = svc_kernal_model.score(X_train, y_train).round(4)\n",
    "svc_kernal_score_test  = svc_kernal_model.score(X_test, y_test).round(4)\n",
    "svc_kernal_AUC         = roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = svc_kernal_pred).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> SVM Model AUC Score </center>\n",
    "\n",
    "Feature Selection | Score |\n",
    "---|-----|\n",
    "Logistic Regression Features | 0.815\n",
    "Pruned Tree Features | 0.818\n",
    "\n",
    "We can see that the the SVM model performed well compared to past models we did. Also the pruned tree features slightly outperformed the logistic model. There is only small difference in performance, but nevertheless the pruned tree required a lot less explanatory variables compared to the logistic variables so this would speed up both performance and the AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Model\n",
    "Using the Bayes model we are able to evaluate the probability of an event happening based on historical information of that event. For example, using this model it would utilize the probability information of how likely it is a customer would successfully cross sell with the wine promotion. With the Bayes Model, it is essentially comparing the probability of the event happening with information, versus without information. The idea behind this is that we have some previous knowledge of the parameters of the model before we have any actual data.\n",
    "\n",
    "(Source: [Towards Data Science](https://towardsdatascience.com/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # bayes model\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             customer_data,\n",
    "#             customer_target,\n",
    "#             test_size    = 0.25,\n",
    "#             random_state = 222,\n",
    "#             stratify     = customer_target)\n",
    "\n",
    "# #import necessary package\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# #instantiate model\n",
    "# bayes_model = GaussianNB()\n",
    "\n",
    "# # fit model\n",
    "# bayes_fit = bayes_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # PREDICTING on new data\n",
    "# bayes_pred = bayes_model.predict(X_test)\n",
    "\n",
    "\n",
    "# # SCORING the model\n",
    "# print('Training ACCURACY:', bayes_model.score(X_train, y_train).round(4))\n",
    "# print('Testing  ACCURACY:', bayes_model.score(X_test, y_test).round(4))\n",
    "# print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "#                                           y_score = bayes_pred).round(4))\n",
    "\n",
    "# bayes_score_train = bayes_model.score(X_train, y_train).round(4)\n",
    "# bayes_score_test  = bayes_model.score(X_test, y_test).round(4)\n",
    "# bayes_AUC         = roc_auc_score(y_true  = y_test,\n",
    "#                                           y_score = bayes_pred).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Bayes Model AUC Score </center>\n",
    "\n",
    "Feature Selection | Score |\n",
    "---|-----|\n",
    "Logistic Regression Features | 0.7906\n",
    "Pruned Tree Features |0.7467\n",
    "\n",
    "Interesting to see a relatively large drop in the AUC score from logistic regression to the pruned tree, almost 0.5 drop which would significantly harm the model performance if we choose the pruned trees features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random Forest\n",
    "Random Forest is an ensemble method, that makes multiple decision trees at random. Each individual tree would output a results based on the model parameters, which leads the most frequent results being outputted as the overall performance. The goal is to understand that decision trees that are constantly being tested will output a stronger result in comparison to one individual tree.\n",
    "\n",
    "(Source: <a href=\"https://www.kdnuggets.com/2017/10/random-forests-explained.html\">KD Nugget</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # This is the exact code we were using before\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             customer_data,\n",
    "#             customer_target,\n",
    "#             test_size    = 0.25,\n",
    "#             random_state = 222,\n",
    "#             stratify     = customer_target)\n",
    "\n",
    "# #import necessary packages\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # instantiate\n",
    "# rdm_forest_model = RandomForestClassifier(n_estimators = 5, \n",
    "#                                           criterion = 'entropy',\n",
    "#                                           random_state = 222\n",
    "#                                          )\n",
    "\n",
    "# #fit model\n",
    "# rdm_forest_fit = rdm_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# # PREDICTING on new data\n",
    "# rdm_forest_pred = rdm_forest_model.predict(X_test)\n",
    "\n",
    "\n",
    "# # SCORING the model\n",
    "# print('Training ACCURACY:', rdm_forest_model.score(X_train, y_train).round(4))\n",
    "# print('Testing  ACCURACY:', rdm_forest_model.score(X_test, y_test).round(4))\n",
    "# print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "#                                           y_score = rdm_forest_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Random Forest Model AUC Score </center>\n",
    "\n",
    "Feature Selection | Score |\n",
    "---|-----|\n",
    "Logistic Regression Features | 0.7378\n",
    "Pruned Tree Features |0.7214\n",
    "\n",
    "Here we see the logistic model features selection performing better than the pruned tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Comparison\n",
    "Now we have all our model results, we can compare how each performed on both explanatory variable sets.\n",
    "\n",
    "Model | Logistic Regression Features | Pruned Tree Features\t|\n",
    "------|------------------------------|----------------------|\n",
    "Logistic Regression | 0.7242 | 0.6961|\n",
    "KNN Non-Standardized | 0.7557 |0.7721|\n",
    "CART - Pruned | 0.7314 | 0.7162|\n",
    "SVM | 0.815 | 0.818|\n",
    "Bayes |0.7906 | 0.7467|\n",
    "Random Forest |  0.7378 | 0.7214|\n",
    "\n",
    "Overall we see that the explanatory variables from the Stats Model Logistic Regression performed better on Scikit Learn Models:\n",
    "\n",
    "    - Logistic Regression\n",
    "    - CART - Pruned\n",
    "    - Bayes\n",
    "    - Random Forest\n",
    "    \n",
    "and the Pruned Tree explanatory variables performed better on:\n",
    "\n",
    "    - SVM\n",
    "    - KNN Non-Standardized\n",
    "    \n",
    "### Top Model\n",
    "In the end the final model with the highest AUC score was the Support Vector Machine (SVM), which it outperformed all the models consistently, with the pruned tree explanatory variables having a slightly higher score.\n",
    "\n",
    "    Support Vector Machine (SVM) Model AUC Score: 0.818\n",
    "    \n",
    "    \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Project Overview\n",
    "\n",
    "### Insight #1\n",
    "\n",
    "Users that signed up for their accounts using their professional work emails had shown strong significance in model performance. Users with professional emails totaled 36% (696 users) of the entire customer base and converted 80% of the time to the cross-sell campaign. \\\n",
    "\n",
    "With this information, we can potentially determine that users signing up with their work email could be earning more income, have less time to cook and prepare their food, and are at minimum age to consume wine. These users would be getting a more efficient cooking experience using Apprentice Chefs product and would purchase wine more frequently.\n",
    "\n",
    "### Insight #2\n",
    "\n",
    "Classifying customers into discount levels, Basic, Premium, and no discount, there were no users in the basic discount, compared to 72% (1400) of users with a premium discount, and 546 users with no discounts.  Surprisingly the wine promotion had the same success rate, 68% for both premium and no discount groups. \n",
    "\n",
    "One would expect that customers with discounts would be more attracted to the wine promotion. However, the company could be losing more money, offering a premium discount to customers who are not converting the cross-sell wine promotion.  Financially the discounts could be counterproductive for cross-sell success.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "The business opportunity for Apprentice Chef would be to increase its presence in corporate partnerships. We saw users with professional emails, produce more robust results in converting the wine promotion. \n",
    "\n",
    "The startup culture, in particular, have great meal benefits, providing their employee's food at lunch and sometimes at night post work. On average, Apprentice Chef's meals cost $16.50, and a company spends $15-17 per employee meal (HighFive, 2019). Apprentice Chef should be approaching companies to provide exclusive partnership prices to supply their meals to their employees, so instead of staying back to eat lunch or dinner, they can head home and cook for themselves and their family. \n",
    "\n",
    "Competitor Blue Apron, provide exclusive discounts for corporates depending on meal frequency and company size (Blue Apron). With these employees spending more time at home and enjoying the services from Apprentice Chef, the wine promotion could be easier to convert. Given that their employers will supply the employees Apprentice Chef meals, the employee could add the wine promotion out of their pocket and wouldn't incur that much cost overall.\n",
    "Working with companies to supply meals to their employees could monumentally increase new users, grow revenue, and cross-sell success of the wine promotion.\n",
    "\n",
    "------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sources\n",
    "\n",
    "1.\tBlue Apron. (n.d.). Retrieved from https://www.blueapron.com/pages/corporate-sales\n",
    "2.\tBrownlee, J. (2019, December 26). Discover Feature Engineering, How to Engineer Features and How to Get Good at It. Retrieved from https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "3.\tChakure, A. (2019, July 7). Decision Tree Classification. Retrieved from https://towardsdatascience.com/decision-tree-classification-de64fc4d5aac\n",
    "4.\tGoogle Developers. (n.d.). Classification: ROC Curve and AUC  |  Machine Learning Crash Course. Retrieved from https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "5.\tHighfive. (2019, August 24). The Value of Providing Lunch for Employees. Retrieved from https://highfive.com/blog/value-of-providing-lunch-for-employees\n",
    "6.\tMcleod, S. (2019, May 20). P-values and statistical significance. Retrieved from https://www.simplypsychology.org/p-value.html\n",
    "7.\tMinitab. (n.d.). Three Common P-Value Mistakes You'll Never Have to Make. Retrieved from https://blog.minitab.com/blog/understanding-statistics/three-common-p-value-mistakes-youll-never-have-to-make\n",
    "8.\tPrinceton. (n.d.). DSS - Interpreting Regression Output. Retrieved from https://dss.princeton.edu/online_help/analysis/interpreting_regression.htm\n",
    "9.\tReinstein, I. (n.d.). Random Forests, Explained. Retrieved from https://www.kdnuggets.com/2017/10/random-forests-explained.html\n",
    "10.\tRen, E. (2019, April 1). Fundamental Techniques of Feature Engineering for Machine Learning. Retrieved from https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\n",
    "11.\tSrivastava, T. (2019, September 3). Introduction to KNN, K-Nearest Neighbors : Simplified. Retrieved from https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/\n",
    "12.\tStephanie. (2017, October 12). Stratification: Definition. Retrieved from https://www.statisticshowto.datasciencecentral.com/stratification-definition/\n",
    "13.\tZornoza, J. (2019, September 5). Probability Learning II: How Bayes' Theorem is applied in Machine Learning. Retrieved from https://towardsdatascience.com/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
